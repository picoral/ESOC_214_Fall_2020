[["index.html", "ESOC 2014 Introduction to Data Science Fall 2020 Module 1 Syllabus 1.1 Course Description 1.2 Course Objectives 1.3 Learning Outcomes 1.4 A Few Words on R and Coding 1.5 A Few Words on Technology 1.6 Readings 1.7 Assignments with Grade Breakdown 1.8 Requirements for the Course 1.9 Course Schedule 1.10 Final Project 1.11 Honors Students‚Äô Requirements 1.12 Student Accommodations 1.13 Attendance, Due Dates, and Missing Work 1.14 Course Conduct and Campus Policies 1.15 Code of Conduct 1.16 How to Ask For Help", " ESOC 2014 Introduction to Data Science Fall 2020 Adriana Picoral, PhD (she/her) adrianaps@email.arizona.edu 2020-11-12 Module 1 Syllabus The University of Arizona sits on the homelands of the Tohono O‚Äôodham and Pascua Yaqui, whose care and keeping of these lands allows us to be here today. Territory acknowledgements are one small part of disrupting and dismantling colonial structures. This syllabus is subject to change if need arises. There are two sections of this course Tuesday &amp; Thursday 12:30pm - 1:45pm Final Exam Date: December 16 (Wednesday) 1:00pm - 3:00pm Tuesday &amp; Thursday 2:00pm - 3:15pm Final Exam Date: December 14 (Monday) 3:30pm - 5:30pm Office Hours/Free help session/Work time Tuesday 9:30am - 11:00am &amp; Wednesday 1:00pm - 2:30pm 1.1 Course Description This course provides an introduction to the various skills and considerations required for data management and analysis in business, education, and science. Particular attention will be given to learning how to use the free and open-source computing environment R, focusing on the tidyverse package for data science. This course is designed to be interactive and hands-on. 1.2 Course Objectives This course aims at providing students with an understanding of the various steps in the data science workflow. Students will engage in data wrangling and exploration to provide answers to questions about the data, using the R programming language. During the semester students will work on an individual data science project to be presented to the class. 1.3 Learning Outcomes At the end of this course, students will be able to: Apply the different steps of data science as a process to derive knowledge from data 1.1. form the question to be answered 1.2. acquire the data to answer question 1.3. transform and tidy data so that data analysis is possible 1.4. explore data with understanding as the goal, which includes data visualization 1.5. communicate data analysis results Demonstrate proficiency of the steps 1.3 - 1.5 above in the R programming language and R Markdown Identify and apply professional standards regarding all aspects of data ethics and privacy, including how data are stored, used, managed, analyzed, and presented Demonstrate knowledge of what a data scientist is and what a career in data science requires in terms of education, and set goals and make plans in case they want to pursue data science beyond the completion of this course Please refer to the department‚Äôs undergraduate student competencies to find out how this course‚Äôs learning outcomes fit into your broad education goals. 1.4 A Few Words on R and Coding This course will be based around the programming language R which we will use within the integrated development environment (IDE) R Studio. For many of you this will be the first time programming, AND THAT‚ÄôS OK! This course is intended for beginners, and we will actively focus on building up your R skills over the course of the semester. Of course, there will still be challenges along the way, but you will rapidly figure out how to solve your own problems as well as to apply your current knowledge to new and exciting questions. If you are struggling I highly encourage you to take advantage of my free help sessions (see times above). Of course, Google is always a super helpful way to get insight into coding problems. Our class Slack channel will also be there so you can help each other out. You might want to watch Roger Peng‚Äôs video on how to get help, which contains guidelines on what information to provide when asking a question in a public forum. I also want to note that I highly encourage you to help each other, as data scientists are rarely working in isolation. This does not mean you can directly share code associated with an assignment (this is a violation of UA‚Äôs Code of Academic Integrity). What it does mean is that it is helpful to talk to each other about problems you encountered, resources you found, or provide helpful tips. Learning to code nowadays is much easier, since a simple Google search will research in a huge amount of code that can solve any number of problems. You may use online resources (e.g., StackOverflow), but we will go over the syntax needed to solve all assignments in class. If you do use any external resources, you must explicitly cite where the code was obtained in your comments (add a direct link to the resource). I‚Äôll be checking for recycled code, and any code you re-used without a proper citation will be treated as plagiarism. 1.5 A Few Words on Technology YOU MUST HAVE ACCESS TO A COMPUTER YOU CAN CODE WITH IN EVERY CLASS! We will be actively coding in R on a daily basis, and not being able to follow along will severely hamper your learning. If you do not have a laptop or yours had troubles at some point during the semester, the library offers fast and free rentals of both macs and PCs: https://new.library.arizona.edu/tech/borrow. You can also take advantage of the multiple computer labs on campus: https://it.arizona.edu/service/oscr-computer-labs You will have access to and will be required to retrieve all course materials from the course page on GitHub. You will need to have R and R Studio installed and functioning by the second day of class. We will go over what these programs are and how to install them in the first week of class. Slack participation is critical! If you are having a coding issue, first try and solve it on your own. If you‚Äôre still struggling, then post it to our Slack. Essentially, if you are about to email me with a homework/class/coding question, post it to Slack first. I‚Äôm not doing this to save me time, but rather because virtually all programmers/coders solve problems by helping each other, and thus I want you to do the same! Please register for our Slack channel. 1.6 Readings There is no required textbook for this class. A few times we will use the book ‚ÄúR for Data Science‚Äù by Hadley Wickham and Garret Grolemund. This book covers how to create full data science pipelines in R (more than we‚Äôll be doing here) and is available free here: https://r4ds.had.co.nz/. Aside from this book, there will be other required readings. I will link these readings for you on this bookdown. Some come from academic journals, and others are news articles that appear in many of the newspapers you read in print and online. For each reading, a word count and an approximate reading time will be provided. Please adjust these approximations to your own reading time, so you can plan accordingly. It is crucial that you read all assigned readings to do well in this class. Anyone who has not done the reading will simply not be able to participate. 1.7 Assignments with Grade Breakdown Activity Total Percent Unit Percent Description Final Project 30% 5% Project Proposal 15% Write-up 10% Oral presentation This will be a full data science project, complete with formal write-up and presentation to the class Midterm 20% 20% Sharing Code during Zoom sections (5) 10% 2% Data Challenges (9) 28% 3.5% Lowest will be dropped. All assignments must completed by the date and time provided in the assignment instructions Class Participation 10% Participation includes both in-class and message board questions, engagement. To get full credit I should see your name or hear you in class once a week. Intro and exit surveys 2% 1% Late assignments within 24 hours of due date and time will get a 20% grade penalty. Assignments submitted 24 hours after the due date and time will not get any credit. If you are unable to complete an assignment on the due date due to an illness or another personal problem, please contact me as soon as possible so we can talk about ways to help you complete that assignment. Any work turned in for this class needs to be distinctly developed for this class, and not work turned in for other classes. Grade Distribution: 90-100% = A ‚Äúexemplary, far beyond reqs/expectations‚Äù 80-89% = B ‚Äúexceeds requirements/expectations‚Äù 70-79% = C ‚Äúmeets requirements/expectations‚Äù 60-69% = D ‚Äúfalls short of requirements/expectations‚Äù &lt; 60% = E ‚Äúrepeat of course needed‚Äù A Note About Final Grades I do not modify final grades. I have designed this course to be highly passable for the new learner assuming they do the modest homework assignments, come to class, and participate. I‚Äôm not a difficult grader, and I build in extensive opportunities for ‚Äòeasy points.‚Äô Given all this, please do not try and ask for a higher grade when end of semester rolls around. 1.8 Requirements for the Course To succeed in this course, 2-3 hours of study time per hour of formal class time (or per unit) are required. This means that in addition to our three hours of formal class meeting time, 6-9 hours a week of study time are needed in order to meet course expectations. These hours should be spent on reading texts, working on your data challenges, researching for new information, or thinking about course content. It‚Äôs important to mention that each lesson builds upon the previous, and thus staying on top of the material is critical to your success. As mentioned before, this class is built specifically for beginners, and plenty of students who have never coded before have done extremely well. But, the reason they did is that they came to class consistently, asked questions when they had an issue, and completed their data challenges. If you miss a class, come to office hours to make up what you missed. I will do everything possible to make sure you succeeded assuming you‚Äôre willing to put in the work! 1.9 Course Schedule Here is the tentative course schedule. Data challenges are always due before the start of class on the associated due date. There will sometimes be other short readings and assignments. These will be posted on D2L directly after the class period in which they are assigned. Week Date Goals Assignment Week 01 2020-08-25 Introductions Syllabus 2020-08-27 Intro do Data Science Data Science workflow Reading: What‚Äôs data science? (20 min) YouTube video Angry Hiring Manager Panel (6.5 min) Survey 1 (10 min) Week 02 2020-09-01 What‚Äôs Data? What does data analysis look like? IDE overview How and Why to Start a Project Basics of R Reading: Data Science examples (8 min), Data Intake (12 min) Install R and RStudio 2020-09-03 Basics of R - basic operations - objects - data types Week 03 2020-09-08 Basics of R - data frames - inspecting data - slicing your data Read A Million Lines of Bad Code (5 min) What is Statistics Good For? (3 min) 2020-09-10 Submitting assignments through GitHub Join our GitHub classroom Week 04 2020-09-15 Installing R Packages Intro to Tidyverse Read Advice to Young (and Old) Programmers: A Conversation with Hadley Wickham (10 min) Submit test assignment 2020-09-17 Tidyverse Week 05 2020-09-22 Data Wrangling Data Challenge 1 2020-09-24 Data Wrangling Week 06 2020-09-29 Intro to Data Visualization Data Challenge 2 2020-10-01 Data Visualization Week 07 2020-10-06 Data Visualization Data Challenge 3 2020-10-08 Data Visualization Week 08 2020-10-13 Data analysis case study 1 Data Challenge 4 2020-10-15 Data analysis case study 1 Week 09 2020-10-20 MIDTERM - Study Guide on D2L Data Challenge 5 2020-10-22 Data analysis case study 2 Week 10 2020-10-27 Data analysis case study 2 2020-10-29 Getting Data Data Challenge 6 Week 11 2020-11-03 Data analysis case study 3 Deadline to meet about final project 2020-11-05 Data analysis case study 3 Project Proposal Week 12 2020-11-10 Markdown Data Challenge 7 2020-11-12 Markdown Week 13 2020-11-17 Full data analysis case study 4 Data Challenge 8 2020-11-19 Full data analysis case study 4 Week 14 2020-11-24 Happy Thanksgiving! üåΩü¶Éüè° Data Challenge 9 2020-11-26 Happy Thanksgiving! üåΩü¶Éüè° Week 15 2020-12-01 Written and Oral Communication in Data Science 2020-12-03 Written and Oral Communication in Data Science Week 16 2020-12-08 Preparing for Final Presentations Wrap-up For more information about dates including holidays, check UArizona‚Äôs Academic Calendar. Why am I using YYYY-MM-DD date format? ISO 8601 ‚Äì xkcd 1.10 Final Project There is a final project in place of a final exam for this class. You will find your own dataset that helps you answer a question that you‚Äôre interested in. You‚Äôll bring these data into R, explore it, clean it, make features, and run an analysis that allows you to answer your question. You will be graded on the completed R script as well as your presentation of the data. The presentation will last 3-4 minutes, and will take place on the day of the final exam (in place of the exam). University policy on final examinations can be found here: https://www.registrar.arizona.edu/courses/final-examination-regulations-and-information 1.11 Honors Students‚Äô Requirements Students wishing to take this course for Honors Credit should email me to set up an appointment to discuss the terms of the contact and to sign the Honors Course Contract Request Form. The form is available at https://honors.arizona.edu/academics/honors-contracts. Students earning credit with the University of Arizona Honors College will be held to the following enhancements: Honors students will be required to create an academic poster based on their final project, and then present this poster at the iSchool‚Äôs iShowcase at the end of the semester. Creating a poster will require extra work to ensure clarity of logic, having a well-defined question and approach, and the creation of quality visuals. Guidelines on how to create an engaging academic poster can be found here: https://guides.nyu.edu/posters. Note: The iShowcase is at the end of the semester, but before finals when the regular class will have the project due. Thus, you will have to be ahead of schedule a bit to meet your honors requirement. Honors students will also be expected to informally ‚Äòjournal‚Äô about the course each week. Each week, that is, students will be required to write a five-sentence paragraph reflecting on some issue or moment that has arisen in our readings or discussions (e.g., the problem with particular terms or some philosophical or practical dilemma). Ultimately, if offering a paragraph each week, honors students will have written roughly 15 reflective paragraphs for the semester. This must be emailed directly to me by Sunday 5pm each week. 1.12 Student Accommodations It is the University‚Äôs goal that learning experiences be as accessible as possible. If you anticipate or experience physical or academic barriers based on disability or pregnancy, please let me know immediately so that we can discuss options. You are also welcome to contact Disability Resources (520-621-3268) to establish reasonable accommodations. For additional information on Disability Resources and reasonable accommodations, please visit http://drc.arizona.edu/. 1.13 Attendance, Due Dates, and Missing Work Missed class assignments or exams cannot be made up without a well-documented, verifiable, excuse (for example, a physician‚Äôs medical excuse). Indeed, due dates are firm, and late work will be accepted only with a verifiable and valid excuse. The UA policy regarding absences for any sincerely held religious belief, observance or practice will be accommodated where reasonable, http://policy.arizona.edu/human-resources/religious-accommodation-policy. Absences pre-approved by the UA Dean of Students (or Dean designee) will be honored. https://deanofstudents.arizona.edu/absences Arriving late and leaving early is extremely disruptive to others in the class. Please avoid this kind of disruption. The UA‚Äôs policy concerning Class Attendance and Administrative Drops is available at: https://catalog.arizona.edu/policy/class-attendance-participation-and-administrative-drop 1.14 Course Conduct and Campus Policies It‚Äôs important to be familiar with all campus policies. Students are encouraged to share intellectual views and discuss freely the principles and applications of course materials. However, graded work/exercises must be the product of independent effort unless otherwise instructed. Students are expected to adhere to the UA Code of Academic Integrity as described in the UA General Catalog. See: http://deanofstudents.arizona.edu/academic-integrity/students/academic-integrity. It is the University‚Äôs goal that learning experiences be as accessible as possible. If you anticipate or experience physical or academic barriers based on disability or pregnancy, please let me know immediately so that we can discuss options. You are also welcome to contact Disability Resources (520-621-3268) to establish reasonable accommodations. For additional information on Disability Resources and reasonable accommodations, please visit http://drc.arizona.edu/. The UA Threatening Behavior by Students Policy prohibits threats of physical harm to any member of the University community, including to oneself. See http://policy.arizona.edu/education-and-student-affairs/threatening-behavior-students. All student records will be managed and held confidentially. http://www.registrar.arizona.edu/personal-information/family-educational-rights-and-privacy-act-1974-ferpa?topic=ferpa The University is committed to creating and maintaining an environment free of discrimination; see http://policy.arizona.edu/human-resources/nondiscrimination-and-anti-harassment-policy. Information contained in this syllabus, other than the grade and absence policy, may be subject to change without advance notice as deemed appropriate by the instructor. 1.15 Code of Conduct This code of conduct is based on GitHub Community Guidelines. One of the goals of this course is to get you familiar with the data science community, and how people work and learn better together. This is a community we build together, and we need everybody‚Äôs help to make it better each day. 1.15.1 Our Pledge In the interest of fostering an open and welcoming environment, we as instructor and students pledge to making participation in our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Be welcoming and open-minded. Although this is an intro course, like in any other learning setting, we have people at different levels of experience. Other people may not have the same experience level or background as you, but that doesn‚Äôt mean they don‚Äôt have good ideas to contribute. I encourage you to be welcoming to everyone, from more advanced coders to those just getting started. We can all learn from each other. Respect each other. Nothing sabotages healthy conversation like rudeness. Be civil and professional, and don‚Äôt post or say anything that a reasonable person would consider offensive, abusive, or hate speech. Don‚Äôt harass or grief anyone. Treat each other with dignity and consideration in all interactions. You may wish to respond to something by disagreeing with it. That‚Äôs fine. But remember to criticize ideas, not people. Avoid name-calling, ad hominem attacks, responding to a post‚Äôs tone instead of its actual content, and knee-jerk reactions. Instead, provide reasoned counter-arguments that improve the conversation. Communicate with empathy. Disagreements or differences of opinion are a fact of life. Being part of a community means interacting with people from a variety of backgrounds and perspectives (and we are all better because of this variety), many of which may not be your own. If you disagree with someone, try to understand and share their feelings before you address them. This will promote a respectful and friendly atmosphere where people feel comfortable asking questions, participating in discussions, and making contributions. Be clear and stay on topic. The goal of this course is to learn about data science and how to do data science with R. Off-topic comments are a distraction (sometimes welcome, but usually not) from getting work done and being productive. Staying on topic helps produce positive and productive discussions. Additionally, as this class will be conducted online, you might not have met each other in person. Communicating on the internet can be awkward, even when you already know people. It‚Äôs hard to convey or read tone, and sarcasm is frequently misunderstood. Try to use clear language, and think about how it will be received by the other person. 1.15.2 Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others‚Äô private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting 1.15.3 Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting your instructor at adrianaps@email.arizona. Your instructor will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. Your instructor is obligated to maintain confidentiality with regard to the reporter of an incident. 1.15.4 Attribution This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4 1.16 How to Ask For Help We‚Äôll see in this course that a key skill that you should develop as a data science is the ability to find solutions to problems. Knowing how to get help is part of that skill. 1.16.1 Before You ask for help Check for typos. One of the most common causes of errors are typos, which usually throw an error such as Error in _____ : could not find function ‚Äú_____‚Äù due to a function being misspelled Check loaded packages. You also get errors like Error in data %&gt;% summary() : could not find function ‚Äú%&gt;%‚Äù when you failed to load a package. Read the error message. Don‚Äôt ignore what R is telling you. Be aware that red text that appears in your console is not alwayws indication of errors. Sometimes it‚Äôs just a warning. Google is your friend. Copy and paste the exact error message on a Google search. (this step also includes read the documentation on the package you‚Äôre trying to use). If you are still stuck, you an always try rubber duck debugging. Describe the problem aloud, explaining it line-by-line, to a rubber duck or another person (who might not have any experience with programming of data science). This is also a good preparation step to asking other people for help (next section). 1.16.2 Ask other people for help Like mentioned before, you should ask your peers for help before you ask your instructor. Relying on a single person to solve all of your problems is dangerous, because that person won‚Äôt be available throughout your career as a data scientist. Check our Slack to see if someone else has asked a question similar to yours, and whether there‚Äôs a solution posted for it. Be precise and informative. The more context you can provide about what you‚Äôre trying to do and what errors you‚Äôre getting, the better. Also describe the steps you took to try to solve the problem yourself. 1.16.3 List of Resources Getting Help with R Roger Peng‚Äôs How To Get Help video Rubber Duck Debugging "],["whats-ds.html", "Module 2 What‚Äôs data science? 2.1 Before class #1 2.2 What‚Äôs data science? 2.3 What does a data scientist do? 2.4 Data Science Workflow 2.5 Before class #2 2.6 What‚Äôs data? 2.7 What does data analysis look like?", " Module 2 What‚Äôs data science? 2.1 Before class #1 Required external reading for this module: What‚Äôs data science? (4,660 words, approx. 20 minutes of reading time) Watch the YouTube video Angry Hiring Manager Panel from 10:18 to 16:48 (6.5 minutes) and list the skills they mention as important to have in a data science position. Fill out Survey 1 (10 min) 2.2 What‚Äôs data science? Data science is one of the fields with the highest demand, with prospects of increased demand for the next decade (Kross et al. 2020; Hadavand, Gooding, and Leek 2018). Interestingly, the data scientist title was invented in 2008, and the median base salary for a data scientist surpassed $100,000 in the United States in 2019 (Robinson and Nolis 2020). CHALLENGE Based on your own experience and on your reading for this module, in your groups discuss the following question: What is data science? 2.3 What does a data scientist do? Data science is an interdisciplinary field, and as such data scientists hold jobs with a broad range of skills, from statistics to communication. A quick search for data science jobs reveals this long list of skills. However, no single data scientist has all skills listed for different data science jobs. Instead, each data scientist specializes in different skills (Robinson and Nolis 2020). CHALLENGE Make a list of skills listed on data science job announcements and in the video you just watched. Based on these, discuss the following questions in your group: Which skills do you already have? At what level of proficiency? Which skills are you interested in developing further? Based on the skills you already have, and the skills you want to acquire, what type of job in data science would you be interested in? 2.4 Data Science Workflow The basic data science workflow involve three main parts: The Question: Form the question you want to answer. Many times you will be given a question, and you have to ‚Äútranslate‚Äù it so you can answer it with your data analysis. Data Acquisition: data file, database, or web API Data Wrangling: import + tidy data + transform (Grolemund and Wickham 2018) Data Exploration: transform + visualize + model + repeat (Grolemund and Wickham 2018) Results Communication: visualize + write + knit (Grolemund and Wickham 2018) Figure 2.1: typical data science project model (Grolemund and Wickham, 2018) CHALLENGE In your groups, based on your own intuition and experience, and based on the Introduction to R for Data Science book (Grolemund and Wickham 2018), summarize what each of the following steps means: Tidy Transform Visualize Model Communicate We will approach the entire data science workflow in this course (but not necessarily every step listed), not in this order. We start with step 3 (Data Wrangling) and 4 (Data Exploration), before we address step 2 (Data Acquisition) and step 5 (Communication) CHALLENGE Go back to the list of skills and job positions we discussed (based on the reading and the video): Which steps in the data science workflow correspond to the job skills we talked about? 2.5 Before class #2 Please fill out Survey 1 (10 min). Reading: Data Science examples (1,0333 words, 8 min) Reading: Data Intake (1,686 words, 12 min) 2.6 What‚Äôs data? CHALLENGE In your small group, discuss the examples provided in the excerpt from ‚ÄúExecutive Data Science‚Äù (Caffo, Peng, and Leek 2016). Is data science about ‚Äúdata‚Äù? Why or why not? Why did Netflix end up not implementing the best solution from the Netflix prize challenge? What data was used in each of the examples provided in the reading? What is data? (come up with a definition). Examples of what data might look like. Structured data (rare): State School Year Average Tuition Nevada 2004-05 3621.392 Nevada 2005-06 3687.290 Florida 2004-05 3848.201 Florida 2007-08 3879.416 Florida 2006-07 3887.656 Florida 2005-06 3924.234 Wyoming 2008-09 3928.671 Wyoming 2007-08 4071.898 Wyoming 2004-05 4086.351 Wyoming 2006-07 4122.205 CHALLENGE Which of the columns (or variables) in the data frame above are categorical, which are quantitative? Structured, but messy data (more common): State 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 2014-15 2015-16 Alabama 5682.838 5840.550 5753.496 6008.169 6475.092 7188.954 8071.134 8451.902 9098.069 9358.929 9496.084 9751.101 Alaska 4328.281 4632.623 4918.501 5069.822 5075.482 5454.607 5759.153 5762.421 6026.143 6012.445 6148.808 6571.340 Arizona 5138.495 5415.516 5481.419 5681.638 6058.464 7263.204 8839.605 9966.716 10133.503 10296.200 10413.844 10646.278 Arkansas 5772.302 6082.379 6231.977 6414.900 6416.503 6627.092 6900.912 7028.991 7286.580 7408.495 7606.410 7867.297 California 5285.921 5527.881 5334.826 5672.472 5897.888 7258.771 8193.739 9436.426 9360.574 9274.193 9186.824 9269.844 Colorado 4703.777 5406.967 5596.348 6227.002 6284.137 6948.473 7748.201 8315.632 8792.856 9292.954 9298.599 9748.188 Connecticut 7983.695 8249.074 8367.549 8677.702 8720.976 9371.019 9827.013 9736.431 10036.627 10453.110 10663.995 11397.337 Delaware 8352.890 8610.597 8681.846 8945.801 8995.473 9987.183 10534.181 11026.241 11362.690 11502.524 11514.660 11676.216 Florida 3848.201 3924.234 3887.656 3879.416 4150.004 4783.032 5510.659 5940.945 6494.901 6451.664 6345.000 6360.159 Georgia 4298.040 4492.167 4584.268 4790.266 4831.365 5549.913 6428.007 7709.284 7853.257 7992.390 8063.014 8446.961 user_id screen_name text reply_to_screen_name 6.331283e+07 blagogirl (???) The illiterate calling Iran out? 80 million bounty on Trumps head? realTuckFrumper 6.331283e+07 blagogirl (???) Iran does NOT fear Trump. They realize what OUR country is dealing with. ‚ÄúThe White House is inflicted with mental retardation‚Äù JonHutson 1.125104e+18 dl_kirkwood I‚Äôm afraid 11 soldiers had to be shipped out from the Iran hit after all with traumatic brain injuries. Seems the Military does not notify homeland unless a soldiers is shipped out for the injury. So, Trump did not know for a week. https://t.co/HdBNbKClBl NA 2.820552e+07 djbarro (???) Are you going to carry a sign supporting the women in Iran brave enough to remove their hijabs and go to prison? GloriaAllred 1.506314e+08 kizu91 US‚Ä¶Special‚Ä¶Representative‚Ä¶Hold‚Ä¶Press‚Ä¶Briefing‚Ä¶Situation in‚Ä¶Iran‚Ä¶Video‚Ä¶first‚Ä¶week‚Ä¶January‚Ä¶saw‚Ä¶drastic‚Ä¶spike‚Ä¶tensions‚Ä¶Washington‚Ä¶Tehran‚Ä¶President‚Ä¶Donald Trump‚Ä¶order‚Ä¶assassination‚Ä¶elite‚Ä¶Quds‚Ä¶Force‚Ä¶commander‚Ä¶Qasem‚Ä¶Soleimani‚Ä¶Iraq NA 1.506314e+08 kizu91 crash‚Ä¶land‚Ä¶collide‚Ä¶plane‚Ä¶aircraft‚Ä¶all‚Ä¶176‚Ä¶people‚Ä¶on board‚Ä¶Iran‚Ä¶missile‚Ä¶attack‚Ä¶US‚Ä¶base‚Ä¶Iraq‚Ä¶rocket‚Ä¶Western‚Ä¶Sahara‚Ä¶Suriname‚Ä¶Colombia‚Ä¶Dominica‚Ä¶Australia‚Ä¶Anguilla‚Ä¶Guadeloupe‚Ä¶Uruguay‚Ä¶Cyprus‚Ä¶Namibia‚Ä¶Brazil‚Ä¶Paraguay‚Ä¶Denmark‚Ä¶55 NA 1.506314e+08 kizu91 Iran‚Ä¶MP‚Ä¶Urge‚Ä¶Gov‚Äôt‚Ä¶Expel‚Ä¶UK‚Ä¶Envoy‚Ä¶Consider‚Ä¶Downgrading‚Ä¶Diplomatic‚Ä¶Ties‚Ä¶Alleged‚Ä¶Meddling‚Ä¶envoy‚Ä¶Robert Macaire‚Ä¶detained‚Ä¶days‚Ä¶ago‚Ä¶alleged‚Ä¶participation‚Ä¶unsanctioned‚Ä¶protest‚Ä¶Tehran‚Ä¶down‚Ä¶Ukraine‚Ä¶Boeing‚Ä¶737‚Ä¶release‚Ä¶15‚Ä¶minutes NA 1.506314e+08 kizu91 Government‚Ä¶Supporter‚Ä¶Gather‚Ä¶Tehran‚Ä¶13‚Ä¶.Friday‚Ä¶Prayer‚Ä¶Video‚Ä¶Iran‚Ä¶gather‚Ä¶rally‚Ä¶commemorate‚Ä¶kill‚Ä¶fatal‚Ä¶crash‚Ä¶land‚Ä¶collide‚Ä¶Ukraine‚Ä¶Boeing‚Ä¶plane‚Ä¶aircraft‚Ä¶shot‚Ä¶down‚Ä¶missile‚Ä¶rocket‚Ä¶January‚Ä¶Imam‚Ä¶Khomeini‚Ä¶International‚Ä¶Airport‚Ä¶16 NA 1.506314e+08 kizu91 British‚Ä¶Treasury‚Ä¶Expand‚Ä¶Hezbollah‚Ä¶Asset‚Ä¶Freeze‚Ä¶UK‚Ä¶government‚Ä¶approved‚Ä¶measure‚Ä¶follow‚Ä¶heat‚Ä¶conflict‚Ä¶United States‚Ä¶Islamic‚Ä¶Republic‚Ä¶Iran‚Ä¶Trump‚Ä¶Administration‚Ä¶target‚Ä¶assassination‚Ä¶high-profile‚Ä¶military‚Ä¶general‚Ä¶early‚Ä¶January‚Ä¶film NA 7.297365e+17 SwmpladySH Hackers Are Coming for the 2020 Election ‚Äî And We‚Äôre Not Ready https://t.co/q82kNu9gMd via (???) NA Textual Data (always messy): ## [1] &quot;CHAPTER I&quot; ## [2] &quot;&quot; ## [3] &quot;&quot; ## [4] &quot;Emma Woodhouse, handsome, clever, and rich, with a comfortable home&quot; ## [5] &quot;and happy disposition, seemed to unite some of the best blessings of&quot; ## [6] &quot;existence; and had lived nearly twenty-one years in the world with very&quot; ## [7] &quot;little to distress or vex her.&quot; ## [8] &quot;&quot; ## [9] &quot;She was the youngest of the two daughters of a most affectionate,&quot; ## [10] &quot;indulgent father; and had, in consequence of her sister&#39;s marriage, been&quot; ** CHALLENGE ** What data formats are out there in the world. Create a list based on your experience and the excerpt from ‚ÄúModern Data Science with R‚Äù (Baumer, Kaplan, and Horton 2017). 2.7 What does data analysis look like? The way you communicate your data analysis will depend on what question you‚Äôre trying to answer and who your audience is. Here are some of my favorite data analysis reports: Whose (coffee) beans reign supreme? A #tidytuesday static image Women in Space A #tidytuesday static image Which city is faster? A City Cycle Race Shinny app The Physical Traits that Define Men and Women in Literature An interactice website References "],["install-r.html", "Module 3 Exploring our IDE (Rstudio) 3.1 Before class #3 3.2 Why learn R? 3.3 Why use RStudio? 3.4 Create an R Project 3.5 Operations and Objects", " Module 3 Exploring our IDE (Rstudio) 3.1 Before class #3 Install R and RStudio. We are using RStudio as our IDE for this course. If you are running your R code in your computer, you need to install both R and RStudio. Alternatively, you can create a free account at http://rstudio.cloud and run your R code in the cloud. Either way, we will be using the same IDE (i.e., RStudio). What‚Äôs an IDE? IDE stands for integrated development environment, and its goal is to facilitate coding by integrating a text editor, a console and other tools into one window. 3.1.1 I‚Äôve never installed R and RStudio in my computer OR I‚Äôm not sure I have R and RStudio installed in my computer Download and install R from https://cran.r-project.org (If you are a Windows user, first determine if you are running the 32 or the 64 bit version) Download and install RStudio from https://rstudio.com/products/rstudio/download/#download Here‚Äôs a video on how to install R and RStudio on a mac. 3.1.2 I already have R and RStudio installed Open RStudio Check your R version by entering sessionInfo() on your console. The latest release for R was June 22, 2020 (R version 4.0.2 Taking Off Again). If your R version is older than the most recent version, please follow step 1 in the previous section to update R. Check your RStudio version, if your version is older than Version 1.3.x, please follow step 2 in the previous section to update RStudio. How often should I update R and RStudio? Always make sure that you have the latest version of R, RStudio, and the packages you‚Äôre using in your code to ensure you are not running into bugs that are caused by having older versions installed in your computer. When asked, Jenny Bryan summarizes the importance of keeping your system up-to-date saying that ‚ÄúYou will always eventually have a reason that you must update. So you can either do that very infrequently, suffer with old versions in the middle, and experience great pain at update. Or admit that maintaining your system is a normal ongoing activity, and do it more often.‚Äù You can ensure your packages are also up-to-date by clicking on ‚ÄúTools‚Äù on your RStudio top menu bar, and selecting ‚ÄúCheck for Packages Updates‚Ä¶‚Äù 3.2 Why learn R? R is both a programming language and a free software environment for statistical computing and graphics. In addition to being free, here are other reasons to learn R: R is popular. According to Robert A. Muenchen‚Äôs post on the popularity of data science software (which is updated frequently), R is among the top 5 technologies that are mentioned in data science job ads on indeed.com. R is very powerful and versatile. From creating websites (like this bookdown you‚Äôre reading right now) to building machine learning models, R has it all. The R community is active and very supportive. Because R is so popular, there are a number of forums on R. A good way to get a glimpse on how active the R community is to follow #rstats on twitter. 3.3 Why use RStudio? You can just use R, but RStudio is an IDE that makes using R easier and more fun. Some features that make RStudio the IDE that many data scientists use: RStudio is free and open source. RStudio contains a full-feature integrated text editor, with tab-completion, spellcheck, etc. RStudio is a cross-platform interface that looks the same across platforms. RStudio allows you to organize your data science projects so you‚Äôre not always hunting for the right script that goes with the data you want to analyze. (also, it integrates nicely with rmarkdown and knitr) 3.4 Create an R Project In today‚Äôs class, we will focus on situating ourselves around our IDE. For every lesson, we will either start a new R project or open an R project we‚Äôve been working on. Why create a RStudio project? RStudio projects make it easier to keep your projects organized, since each project has their own working directory, workspace, history, and source documents. In other words, it‚Äôs much easier to open an R project and not have to worry about setting your working directory than to try to hunt down your files. Here are the steps we are starting with today: Start a new R project Create a new R script Save that R script as 01-class_one CHALLENGE Take a moment to look around your IDE. What are the main panes on the RStudio interface. What are the 4 main areas of the interface? Can you guess what each area is for? 3.5 Operations and Objects Let‚Äôs start by using R as a calculator. On your console type 3 + 3 and hit enter. 3 + 3 ## [1] 6 What symbols do we use for all basic operations (addition, subtraction, multiplication, and division)? What happens if you type 3 +? Let‚Äôs save our calculation into an object, by using the assignment symbol &lt;-. sum_result &lt;- 3 + 3 Take a moment to look around your IDE once again. What has changed? Now, let‚Äôs use this new object in our calculation sum_result + 3 ## [1] 9 Take a moment to look around your IDE once again. Has anything changed? What else can we do with an object? class(sum_result) ## [1] &quot;numeric&quot; R is primarily a functional programming language. That means that there pre-programmed functions in base R such as class() and that you can also write your own functions (more on that later). Type ?class in your console and hit enter to get more information about this function. CHALLENGE Create an object called daisys_age that holds the number 8. Multiply daisys_age by 4 and save the results in another object called daisys_human_age Imagine I had multiple pets (unfortunately, that is not true, Daisy is my only pet). I can create a vector to hold multiple numbers representing the age of each of my pets. my_pets_ages &lt;- c(8, 2, 6, 3, 1) Take a moment to look around your IDE once again. What has changed? What is the class of the object my_pets_ages? Now let‚Äôs multiply this vector by 4. my_pets_ages * 4 ## [1] 32 8 24 12 4 Errors are pretty common when writing code in any programming language, so be ready to read error messages and debug your code. Let‚Äôs insert a typing error in our previous code: my_pets_ages &lt;- c(8, 2, 6, &#39;3&#39;, 1) CHALLENGE Try to multiply my_pets_ages by 4. What happens? How can we debug our code to find out what is causing the error? "],["r-basics.html", "Module 4 R Basics 4.1 Before Class #4 4.2 Dataframes 4.3 Slicing your dataframe 4.4 Adding new variables (i.e., columns) to your dataframe 4.5 Descriptive stats on dataframes 4.6 Note on coding style", " Module 4 R Basics 4.1 Before Class #4 Read A Million Lines of Bad Code a blog post by David Robinson. (549 words, 5 minutes) Read What is Statistics Good For? (398 words, 3 min) 4.2 Dataframes You will rarely work with individual numeric values, or even individual numeric vectors. Often, we have information organized in dataframes, which is R‚Äôs version of a spreadsheet. Let‚Äôs go back to my imaginary pet‚Äôs ages (make sure you have the correct vector in your global environment). my_pets_ages &lt;- c(8, 2, 6, &#39;3&#39;, 1) my_pets_ages &lt;- as.numeric(my_pets_ages) We will now create a vector of strings or characters that holds my imaginary pets‚Äô names (we have to be careful to keep the same order then the my_pets_ages vector). my_pets_names &lt;- c(&#39;Daisy&#39;, &#39;Violet&#39;, &#39;Lily&#39;, &#39;Iris&#39;, &#39;Poppy&#39;) Let‚Äôs now create a dataframe that contains info about my pets. # create dataframe my_pets &lt;- data.frame(name = my_pets_names, age = my_pets_ages) # print out dataframe my_pets ## name age ## 1 Daisy 8 ## 2 Violet 2 ## 3 Lily 6 ## 4 Iris 3 ## 5 Poppy 1 CHALLENGE There‚Äôs a number of functions you can run on dataframes. Try running the following functions on my_pets: summary() nrow() ncol() dim() What other functions can/do you think/know of? 4.3 Slicing your dataframe There are different ways you can slice or subset your dataframe. You can use indices for rows and columns. my_pets[1,] ## name age ## 1 Daisy 8 my_pets[, 1] ## [1] &quot;Daisy&quot; &quot;Violet&quot; &quot;Lily&quot; &quot;Iris&quot; &quot;Poppy&quot; my_pets[1, 1] ## [1] &quot;Daisy&quot; You can use a column name or a row name instead of an index. my_pets[, &#39;age&#39;] ## [1] 8 2 6 3 1 my_pets[&#39;1&#39;, ] ## name age ## 1 Daisy 8 my_pets[&#39;1&#39;, &#39;age&#39;] ## [1] 8 Or you can use $ to retrieve values from a column. my_pets$age ## [1] 8 2 6 3 1 my_pets$age[1] ## [1] 8 You can also use comparisons to filter your dataframe # get index with which() function which(my_pets$age == 8) ## [1] 1 # use which() inside dataframe indexing my_pets[row_number, column_number] my_pets[which(my_pets$age == 8),] ## name age ## 1 Daisy 8 my_pets[which(my_pets$age == 8), 1] ## [1] &quot;Daisy&quot; my_pets[which(my_pets$age == 8), &#39;name&#39;] ## [1] &quot;Daisy&quot; my_pets[which(my_pets$age == 8),]$name ## [1] &quot;Daisy&quot; CHALLENGE Print out a list of pet names that are older than 3. 4.4 Adding new variables (i.e., columns) to your dataframe So far the my_pets dataframe has two columns: name and age. Let‚Äôs add a third column with the pets‚Äô ages in human years. For that, we are going to use $ on with a variable (or column) name that does not exist in our dataframe yet. We will then assign to this variable the value in the age column multiplied by 4. # create new column called human_years my_pets$human_years &lt;- my_pets$age * 4 # print dataframe my_pets ## name age human_years ## 1 Daisy 8 32 ## 2 Violet 2 8 ## 3 Lily 6 24 ## 4 Iris 3 12 ## 5 Poppy 1 4 Inspect the new my_pets dataframe. What dimensions does it have now? How could you get a list of just the human years values in the data frame? 4.5 Descriptive stats on dataframes Let‚Äôs explore some functions for descriptive statistics. CHALLENGE Try running the following functions on my_pets$age and my_pets$human_years: mean() sd() median() max() min() range() What other functions can/do you think/know of? 4.6 Note on coding style Coding style refers to how you name your objects and functions, how you comment your code, how you use spacing throughout your code, etc. If your coding style is consistent, your code is easier to read and easier to debug as a result. Here‚Äôs some guides, so you can develop your own coding style: The tidyverse style guide Hadley Wickham‚Äôs Advance R coding style Google‚Äôs R Style Guide "],["version-control.html", "Module 5 Version Control 5.1 Before Class #5 5.2 What is version control? 5.3 Submitting assignments", " Module 5 Version Control 5.1 Before Class #5 5.1.1 Install git on your computer Access the git download page and download the appropriate version for your machine. If you have a Windows 10 machine, you can watch this video that shows you how to install Git on windows. When installing, note where it‚Äôs installed (on the ‚ÄúSelect Destination Location‚Äù window) so you can check if you have the correct path to Git set up in RStudio (it‚Äôs usually C:\\Program\\Git). If you have a Mac, you can watch this video that shows you how to install Git on a Mac. 5.1.2 Create a GitHub account Access the GitHub page. Click on ‚ÄúSign Up for GitHub.‚Äù Fill out the ‚ÄúCreate your account‚Äù forms. A verification will be sent to your email address, check your inbox for a ‚ÄúPlease verify your email address‚Äù message. Click on ‚ÄúVerify email address‚Äù button. If you already have a GitHub account, confirm you know your username and password by logging in at GitHub. 5.1.3 Join our GitHub classroom Access our join our GitHub classroom page. A window with information about what GitHub Classroom wants to access from your GitHub profile will appear. Click on ‚ÄúAuthorize github‚Äù. Access our first assignment and click on ‚ÄúAccept this assignment‚Äù 5.1.4 Link RStudio to GitHub Open ‚Äúpreferences‚Äù in RStudio. Click on ‚ÄúGit/SVN‚Äù in the menu on the left. Under ‚ÄúSSH RSA Key:‚Äù click on ‚ÄúCreate RSA Key‚Ä¶‚Äù A window will pop up, click on ‚ÄúCreate‚Äù A new window will pop up, click ‚ÄúClose‚Äù Now there‚Äôs a ‚ÄúView public key‚Äù link next to ‚ÄúSSH RSA Key:‚Äù; click on it Copy key and close the window Go to your GitHub account settings On the menu on the left, click on ‚ÄúSSH and GPC keys‚Äù Click on the ‚ÄúNew SSH Key‚Äù button Choose a title (e.g., RStudio Connection) and copy the key to the ‚Äúkey‚Äù field Click ‚ÄúAdd SSH Key‚Äù 5.2 What is version control? Version control is a best practice for reproducible analyses, and widely used in industry and research (i.e., you will need to know how to use version control in your future job). The purpose of version control is to keep track of changes to your files over time, so that you can recall specific versions at any point in your project. Git is an open source version control software system that is very popular ‚Äì 58% of data scientist use Git (Beckman et al. 2020). There are a number of other version control software available (e.g., Perforce). 5.3 Submitting assignments 5.3.1 Clone assignment repository Go to our first assignment GitHub repository Click on the ‚ÄúCode‚Äù button and copy the git url (e.g., https://github.com/esoc214/test-assignment-yournamehere.git) Open RStudio Go ‚ÄúFile‚Äù &gt; ‚ÄúNew Project‚Ä¶‚Äù In the pop-up window, select ‚ÄúVersion Control‚Äù Then choose ‚ÄúGit‚Äù In the ‚ÄúRepository URL:‚Äù field enter the link to the first assignment repository from your GitHub account. Click ‚ÄúCreate‚Äù 5.3.2 Modify files For the first assignment, which is a test assignment so you‚Äôre all set up to submitting all of your assignments for this class, you need to modify READM.md only. For other assignments you will need to edit .R scripts. 5.3.3 Commit changes On the top right panel in RStudio (i.e., Environment quadrant), click on the ‚ÄúGit‚Äù tab You will see a list of files, indicating which files have been modified (a blue ‚ÄúM‚Äù shows next to modified file). Click on ‚ÄúCommit‚Äù on the top of this tab A new window will pop-up. Stage the files you want to commit (click on the check box next to file) and enter a commit message. Press ‚ÄúCommit‚Äù and if everything looks good, close the commit window. Click ‚ÄúPush‚Äù on top right References "],["intro-to-tidyverse.html", "Module 6 Intro to Tidyverse 6.1 Before Class #6 6.2 What are R Packages? 6.3 Installing Packages 6.4 Before You Load your Data 6.5 What‚Äôs our question again? 6.6 Load Data with Tidyverse 6.7 Inspect Your Data 6.8 The Pipe 6.9 Counting Categorical Variables 6.10 group_by + summarise 6.11 group_by + filter 6.12 Pivot Dataframe 6.13 Separating one categorical column into two 6.14 Example of Plotting 6.15 DATA CHALLENGE 01", " Module 6 Intro to Tidyverse 6.1 Before Class #6 Read Advice to Young (and Old) Programmers: A Conversation with Hadley Wickham by Philip Waggoner (2,599 words, 10 minutes) 6.2 What are R Packages? An R package contains functions, and it might contain data. There are a lot of R packages out here (check the Comprehensive R Archive Network, i.e., CRAN, for a full list). That is one of the beautiful things about R, anyone can create an R package to share their code. 6.3 Installing Packages The function to install packages in R is install.packages(). We will be working with TidyVerse extensively in this course, which is a collection of R packages carefully designed for data science. Open your RStudio. In your console, enter the following to install tidyverse (this may take a while). install.packages(&quot;tidyverse&quot;) You need to install any package only once (remember to check for new package versions and to keep your packages updated). However, with every new R session, you need to load the packages you are going to use by using the library() function (a library is an installed R package in your computer). library(tidyverse) Note that when calling the install.packages() function you need to enter the package name between quotation marks (e.g., ‚Äútidyverse‚Äù). When you call the library() function, you don‚Äôt use quotation marks (e.g., tidyverse). 6.4 Before You Load your Data Although we are working within an R project, which sets the working directory automatically for you, it‚Äôs good practice to check what folder you are working from by calling the getwd() function. getwd() ## [1] &quot;/Users/adriana/Desktop/ESOC214/Fall 2020/bookdown/ESOC_214_Fall_2020&quot; You can list the contents of your working directory by using the dir() function. dir() We are going to create a data folder in our project, to keep things organized. Today we will be working with a data set that contains groundhog day forecasts and temperature. I cleaned up this data set already (no need for data tidying for now). You can now list the contents of your data folder with the dir() function with a string that specifies the folder as a parameter. dir(&quot;data&quot;) ## [1] &quot;clean_beer_awards.csv&quot; ## [2] &quot;elnino.csv&quot; ## [3] &quot;GlobalLandTemperaturesByCountry.csv&quot; ## [4] &quot;GlobalLandTemperaturesByMajorCity.csv&quot; ## [5] &quot;groundhog_day.csv&quot; ## [6] &quot;nfl_salary.xlsx&quot; ## [7] &quot;olympic_history_athlete_events.csv&quot; ## [8] &quot;olympic_history_noc_regions.csv&quot; ## [9] &quot;passwords.csv&quot; ## [10] &quot;spotify_songs_clean.csv&quot; ## [11] &quot;spotify_songs.csv&quot; ## [12] &quot;tweets.tsv&quot; ## [13] &quot;us_avg_tuition.xlsx&quot; 6.5 What‚Äôs our question again? Here‚Äôs what we will focus on answering today, which is an excerpt from the Groundhog Day Forecasts and Temperatures kaggle page. ‚ÄúThousands gather at Gobbler‚Äôs Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn‚Äôt see his shadow, the country should expect warmer temperatures and the arrival of an early spring.‚Äù So, in summary, our question is How accurate is Punxsutawney Phil‚Äôs winter weather forecast? 6.6 Load Data with Tidyverse We will use the read_csv() function from the readr package (which is part of tidyverse) to read data in. Be careful, there‚Äôs a similar function that is read.csv() from base R. We do want to use the function with the _ (i.e., read_csv()) groundhog_predictions &lt;- read_csv(&quot;data/groundhog_day.csv&quot;) ## Parsed with column specification: ## cols( ## Year = col_double(), ## Punxsutawney_Phil = col_character(), ## February_Average_Temperature = col_double(), ## February_Average_Temperature_Northeast = col_double(), ## February_Average_Temperature_Midwest = col_double(), ## February_Average_Temperature_Pennsylvania = col_double(), ## March_Average_Temperature = col_double(), ## March_Average_Temperature_Northeast = col_double(), ## March_Average_Temperature_Midwest = col_double(), ## March_Average_Temperature_Pennsylvania = col_double() ## ) CHALLENGE Reading warnings - R often prints out warnings in red (these are not always errors). What information did you get when loading your data? 6.7 Inspect Your Data As with any other programming language, there are multiple ways to doing anything. As such, there are multiple ways of inspecting your data in R. Here are some of my favorite ways of inspecting my data: # get an overview of the data frame glimpse(groundhog_predictions) ## Rows: 122 ## Columns: 10 ## $ Year &lt;dbl&gt; 1895, 1896, 1897, 1898, 189‚Ä¶ ## $ Punxsutawney_Phil &lt;chr&gt; &quot;No Record&quot;, &quot;No Record&quot;, &quot;‚Ä¶ ## $ February_Average_Temperature &lt;dbl&gt; 26.60, 35.04, 33.39, 35.37,‚Ä¶ ## $ February_Average_Temperature_Northeast &lt;dbl&gt; 15.6, 22.2, 23.6, 24.8, 18.‚Ä¶ ## $ February_Average_Temperature_Midwest &lt;dbl&gt; 21.9, 33.5, 34.7, 33.3, 22.‚Ä¶ ## $ February_Average_Temperature_Pennsylvania &lt;dbl&gt; 17.0, 26.6, 27.9, 26.7, 20.‚Ä¶ ## $ March_Average_Temperature &lt;dbl&gt; 39.97, 38.03, 38.79, 41.05,‚Ä¶ ## $ March_Average_Temperature_Northeast &lt;dbl&gt; 27.6, 25.3, 32.0, 38.0, 29.‚Ä¶ ## $ March_Average_Temperature_Midwest &lt;dbl&gt; 40.2, 36.9, 44.0, 46.0, 38.‚Ä¶ ## $ March_Average_Temperature_Pennsylvania &lt;dbl&gt; 31.3, 27.8, 36.9, 42.0, 34.‚Ä¶ summary(groundhog_predictions) ## Year Punxsutawney_Phil February_Average_Temperature ## Min. :1895 Length:122 Min. :25.23 ## 1st Qu.:1925 Class :character 1st Qu.:31.78 ## Median :1956 Mode :character Median :33.69 ## Mean :1956 Mean :33.80 ## 3rd Qu.:1986 3rd Qu.:36.01 ## Max. :2016 Max. :41.41 ## February_Average_Temperature_Northeast February_Average_Temperature_Midwest ## Min. :10.40 Min. :20.30 ## 1st Qu.:20.02 1st Qu.:29.62 ## Median :22.95 Median :33.20 ## Mean :22.69 Mean :32.69 ## 3rd Qu.:25.98 3rd Qu.:36.30 ## Max. :31.60 Max. :41.40 ## February_Average_Temperature_Pennsylvania March_Average_Temperature ## Min. :15.20 Min. :35.44 ## 1st Qu.:23.60 1st Qu.:39.38 ## Median :26.95 Median :41.81 ## Mean :26.52 Mean :41.70 ## 3rd Qu.:29.80 3rd Qu.:43.56 ## Max. :35.80 Max. :50.41 ## March_Average_Temperature_Northeast March_Average_Temperature_Midwest ## Min. :24.20 Min. :28.50 ## 1st Qu.:29.70 1st Qu.:39.08 ## Median :32.55 Median :42.85 ## Mean :32.37 Mean :42.57 ## 3rd Qu.:34.80 3rd Qu.:45.60 ## Max. :43.40 Max. :56.30 ## March_Average_Temperature_Pennsylvania ## Min. :24.50 ## 1st Qu.:32.95 ## Median :35.85 ## Mean :35.91 ## 3rd Qu.:38.55 ## Max. :47.70 # get variable names colnames(groundhog_predictions) ## [1] &quot;Year&quot; ## [2] &quot;Punxsutawney_Phil&quot; ## [3] &quot;February_Average_Temperature&quot; ## [4] &quot;February_Average_Temperature_Northeast&quot; ## [5] &quot;February_Average_Temperature_Midwest&quot; ## [6] &quot;February_Average_Temperature_Pennsylvania&quot; ## [7] &quot;March_Average_Temperature&quot; ## [8] &quot;March_Average_Temperature_Northeast&quot; ## [9] &quot;March_Average_Temperature_Midwest&quot; ## [10] &quot;March_Average_Temperature_Pennsylvania&quot; names(groundhog_predictions) ## [1] &quot;Year&quot; ## [2] &quot;Punxsutawney_Phil&quot; ## [3] &quot;February_Average_Temperature&quot; ## [4] &quot;February_Average_Temperature_Northeast&quot; ## [5] &quot;February_Average_Temperature_Midwest&quot; ## [6] &quot;February_Average_Temperature_Pennsylvania&quot; ## [7] &quot;March_Average_Temperature&quot; ## [8] &quot;March_Average_Temperature_Northeast&quot; ## [9] &quot;March_Average_Temperature_Midwest&quot; ## [10] &quot;March_Average_Temperature_Pennsylvania&quot; # check the categorical variable unique(groundhog_predictions$Punxsutawney_Phil) ## [1] &quot;No Record&quot; &quot;Full Shadow&quot; &quot;No Shadow&quot; &quot;Partial Shadow&quot; CHALLENGE Which variables are numeric? Which are categorical? 6.8 The Pipe We will be using the package dplyr (which is also part of tidyverse) to do an exploratory analysis of our data. The package dplyr most used function is %&gt;% (called the pipe). The pipe allows you to ‚Äúpipe‚Äù (or redirect) objects into functions. (hint: use ctrl+shift+m or cmd+shift+m as a shortcut for typing %&gt;%). Here‚Äôs how to pipe the avocado_data object into the summary() function # get an overview of the data frame groundhog_predictions %&gt;% summary() ## Year Punxsutawney_Phil February_Average_Temperature ## Min. :1895 Length:122 Min. :25.23 ## 1st Qu.:1925 Class :character 1st Qu.:31.78 ## Median :1956 Mode :character Median :33.69 ## Mean :1956 Mean :33.80 ## 3rd Qu.:1986 3rd Qu.:36.01 ## Max. :2016 Max. :41.41 ## February_Average_Temperature_Northeast February_Average_Temperature_Midwest ## Min. :10.40 Min. :20.30 ## 1st Qu.:20.02 1st Qu.:29.62 ## Median :22.95 Median :33.20 ## Mean :22.69 Mean :32.69 ## 3rd Qu.:25.98 3rd Qu.:36.30 ## Max. :31.60 Max. :41.40 ## February_Average_Temperature_Pennsylvania March_Average_Temperature ## Min. :15.20 Min. :35.44 ## 1st Qu.:23.60 1st Qu.:39.38 ## Median :26.95 Median :41.81 ## Mean :26.52 Mean :41.70 ## 3rd Qu.:29.80 3rd Qu.:43.56 ## Max. :35.80 Max. :50.41 ## March_Average_Temperature_Northeast March_Average_Temperature_Midwest ## Min. :24.20 Min. :28.50 ## 1st Qu.:29.70 1st Qu.:39.08 ## Median :32.55 Median :42.85 ## Mean :32.37 Mean :42.57 ## 3rd Qu.:34.80 3rd Qu.:45.60 ## Max. :43.40 Max. :56.30 ## March_Average_Temperature_Pennsylvania ## Min. :24.50 ## 1st Qu.:32.95 ## Median :35.85 ## Mean :35.91 ## 3rd Qu.:38.55 ## Max. :47.70 The pipe allows us to apply multiple functions to the same object. Let‚Äôs start by selecting one column in our data. groundhog_predictions %&gt;% select(Punxsutawney_Phil) ## # A tibble: 122 x 1 ## Punxsutawney_Phil ## &lt;chr&gt; ## 1 No Record ## 2 No Record ## 3 No Record ## 4 Full Shadow ## 5 No Record ## 6 Full Shadow ## 7 Full Shadow ## 8 No Record ## 9 Full Shadow ## 10 Full Shadow ## # ‚Ä¶ with 112 more rows Now let‚Äôs add another pipe to get unique values in this column. groundhog_predictions %&gt;% select(Punxsutawney_Phil) %&gt;% unique() ## # A tibble: 4 x 1 ## Punxsutawney_Phil ## &lt;chr&gt; ## 1 No Record ## 2 Full Shadow ## 3 No Shadow ## 4 Partial Shadow 6.9 Counting Categorical Variables One of the functions I most use when exploring my data is count(), which you can combine with %&gt;%. groundhog_predictions %&gt;% count(Punxsutawney_Phil) ## # A tibble: 4 x 2 ## Punxsutawney_Phil n ## &lt;chr&gt; &lt;int&gt; ## 1 Full Shadow 100 ## 2 No Record 6 ## 3 No Shadow 15 ## 4 Partial Shadow 1 You can do the same adding group_by() to your pipeline. groundhog_predictions %&gt;% group_by(Punxsutawney_Phil) %&gt;% count() ## # A tibble: 4 x 2 ## # Groups: Punxsutawney_Phil [4] ## Punxsutawney_Phil n ## &lt;chr&gt; &lt;int&gt; ## 1 Full Shadow 100 ## 2 No Record 6 ## 3 No Shadow 15 ## 4 Partial Shadow 1 And instead of count() we can use the summarise() and n() functions. groundhog_predictions %&gt;% group_by(Punxsutawney_Phil) %&gt;% summarise(total = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## Punxsutawney_Phil total ## &lt;chr&gt; &lt;int&gt; ## 1 Full Shadow 100 ## 2 No Record 6 ## 3 No Shadow 15 ## 4 Partial Shadow 1 CHALLENGE This last way of counting categorical variables (with summarise() and n()) outputs a data frame that is slightly different from the previous too. What‚Äôs the difference? 6.10 group_by + summarise The combination of the group_by() and summarise() functions is very powerful. In addition to using the n() function to count how many rows per each category in our categorical variable, we can use other functions with numeric (i.e., quantitative) variable such as sum() and mean(). CHALLENGE Take a moment to revisit the question we want to answer. What do we want to find out? How can we answer our question with this data? What function (e.g., sum(), max(), mean()) do we use to answer our question? With what variables/columns? Complete the code below. groundhog_predictions %&gt;% group_by(Punxsutawney_Phil) %&gt;% summarise(total = n(), ____ = ____(____)) Example of output that you might want to get to answer our question: ## # A tibble: 4 x 4 ## Punxsutawney_Phil total feb_mean_temp mar_mean_temp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full Shadow 100 33.7 41.7 ## 2 No Record 6 31.4 39.1 ## 3 No Shadow 15 35.6 43.0 ## 4 Partial Shadow 1 30.7 41.3 6.11 group_by + filter The output above contains six No Record observations and only one Partial Shadow. We can keep just observations that are Full Shadow and No Shadow by using the filter() function: groundhog_predictions %&gt;% filter(Punxsutawney_Phil == &quot;Full Shadow&quot; | Punxsutawney_Phil == &quot;No Shadow&quot;) %&gt;% count(Punxsutawney_Phil) ## # A tibble: 2 x 2 ## Punxsutawney_Phil n ## &lt;chr&gt; &lt;int&gt; ## 1 Full Shadow 100 ## 2 No Shadow 15 CHALLENGE Add a filter() to your solution from the previous challenge. Example of output that you might want to get: ## # A tibble: 2 x 4 ## Punxsutawney_Phil total feb_mean_temp mar_mean_temp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Full Shadow 100 33.7 41.7 ## 2 No Shadow 15 35.6 43.0 6.12 Pivot Dataframe Another useful function we will be using a lot during this course is pivot_longer(), which pivots (or tilts) some columns in our dataframe so we have one column for each of our variables. Let‚Äôs first select the temperatures for individual regions and create a new dataframe. selected_predictions &lt;- groundhog_predictions %&gt;% select(Year, Punxsutawney_Phil, February_Average_Temperature_Northeast, February_Average_Temperature_Midwest, February_Average_Temperature_Pennsylvania, March_Average_Temperature_Northeast, March_Average_Temperature_Midwest, March_Average_Temperature_Pennsylvania) colnames(selected_predictions) ## [1] &quot;Year&quot; ## [2] &quot;Punxsutawney_Phil&quot; ## [3] &quot;February_Average_Temperature_Northeast&quot; ## [4] &quot;February_Average_Temperature_Midwest&quot; ## [5] &quot;February_Average_Temperature_Pennsylvania&quot; ## [6] &quot;March_Average_Temperature_Northeast&quot; ## [7] &quot;March_Average_Temperature_Midwest&quot; ## [8] &quot;March_Average_Temperature_Pennsylvania&quot; Another way of doing the same thing we just did is by saying the columns we want to eliminate from our selection, using the - (i.e, minus) sign. selected_predictions &lt;- groundhog_predictions %&gt;% select(-February_Average_Temperature, -March_Average_Temperature) colnames(selected_predictions) ## [1] &quot;Year&quot; ## [2] &quot;Punxsutawney_Phil&quot; ## [3] &quot;February_Average_Temperature_Northeast&quot; ## [4] &quot;February_Average_Temperature_Midwest&quot; ## [5] &quot;February_Average_Temperature_Pennsylvania&quot; ## [6] &quot;March_Average_Temperature_Northeast&quot; ## [7] &quot;March_Average_Temperature_Midwest&quot; ## [8] &quot;March_Average_Temperature_Pennsylvania&quot; Now we are ready to pivot_longer() our selected_predictions dataframe. Let‚Äôs examine our selected_predictions dataframe. We want to move all numbers to our numeric variable called temperature, and we want the column names to be another variable called month_region so that are data is tidy. head(selected_predictions) ## # A tibble: 6 x 8 ## Year Punxsutawney_Ph‚Ä¶ February_Averag‚Ä¶ February_Averag‚Ä¶ February_Averag‚Ä¶ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1895 No Record 15.6 21.9 17 ## 2 1896 No Record 22.2 33.5 26.6 ## 3 1897 No Record 23.6 34.7 27.9 ## 4 1898 Full Shadow 24.8 33.3 26.7 ## 5 1899 No Record 18.1 22.2 20 ## 6 1900 Full Shadow 21.4 27.5 24.1 ## # ‚Ä¶ with 3 more variables: March_Average_Temperature_Northeast &lt;dbl&gt;, ## # March_Average_Temperature_Midwest &lt;dbl&gt;, ## # March_Average_Temperature_Pennsylvania &lt;dbl&gt; Again we can list all the columns we want to pivot (i.e., all the columns that are numeric), but we have a smaller number of columns we don‚Äôt want to pivot, so we use the - (minus) symbol with the columns we don‚Äôt want to pivot. selected_predictions %&gt;% pivot_longer(cols = c(-Year, -Punxsutawney_Phil)) ## # A tibble: 732 x 4 ## Year Punxsutawney_Phil name value ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1895 No Record February_Average_Temperature_Northeast 15.6 ## 2 1895 No Record February_Average_Temperature_Midwest 21.9 ## 3 1895 No Record February_Average_Temperature_Pennsylvania 17 ## 4 1895 No Record March_Average_Temperature_Northeast 27.6 ## 5 1895 No Record March_Average_Temperature_Midwest 40.2 ## 6 1895 No Record March_Average_Temperature_Pennsylvania 31.3 ## 7 1896 No Record February_Average_Temperature_Northeast 22.2 ## 8 1896 No Record February_Average_Temperature_Midwest 33.5 ## 9 1896 No Record February_Average_Temperature_Pennsylvania 26.6 ## 10 1896 No Record March_Average_Temperature_Northeast 25.3 ## # ‚Ä¶ with 722 more rows We can specify the column names so they are not just name and value: selected_predictions %&gt;% pivot_longer(cols = c(-Year, -Punxsutawney_Phil), names_to = &quot;month_region&quot;, values_to = &quot;temperature&quot;) ## # A tibble: 732 x 4 ## Year Punxsutawney_Phil month_region temperature ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1895 No Record February_Average_Temperature_Northeast 15.6 ## 2 1895 No Record February_Average_Temperature_Midwest 21.9 ## 3 1895 No Record February_Average_Temperature_Pennsylvania 17 ## 4 1895 No Record March_Average_Temperature_Northeast 27.6 ## 5 1895 No Record March_Average_Temperature_Midwest 40.2 ## 6 1895 No Record March_Average_Temperature_Pennsylvania 31.3 ## 7 1896 No Record February_Average_Temperature_Northeast 22.2 ## 8 1896 No Record February_Average_Temperature_Midwest 33.5 ## 9 1896 No Record February_Average_Temperature_Pennsylvania 26.6 ## 10 1896 No Record March_Average_Temperature_Northeast 25.3 ## # ‚Ä¶ with 722 more rows Let‚Äôs save the result above in a new dataframe. predictions_tidy &lt;- selected_predictions %&gt;% pivot_longer(cols = c(-Year, -Punxsutawney_Phil), names_to = &quot;month_region&quot;, values_to = &quot;temperature&quot;) 6.13 Separating one categorical column into two When we look at our predictions_tidy, we see that the month_region is actually holding two categorical variables. We can separate this column into its two variables with the separate() function. There are four parts to each of the values, the two middle parts are not useful so we name the first part month the last region and the other two not useful parts (the middle two) trash1 and trash2. predictions_tidy_v2 &lt;- predictions_tidy %&gt;% separate(col = month_region, into = c(&quot;month&quot;, &quot;trash1&quot;, &quot;trash2&quot;, &quot;region&quot;)) predictions_tidy_v2 ## # A tibble: 732 x 7 ## Year Punxsutawney_Phil month trash1 trash2 region temperature ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1895 No Record February Average Temperature Northeast 15.6 ## 2 1895 No Record February Average Temperature Midwest 21.9 ## 3 1895 No Record February Average Temperature Pennsylvania 17 ## 4 1895 No Record March Average Temperature Northeast 27.6 ## 5 1895 No Record March Average Temperature Midwest 40.2 ## 6 1895 No Record March Average Temperature Pennsylvania 31.3 ## 7 1896 No Record February Average Temperature Northeast 22.2 ## 8 1896 No Record February Average Temperature Midwest 33.5 ## 9 1896 No Record February Average Temperature Pennsylvania 26.6 ## 10 1896 No Record March Average Temperature Northeast 25.3 ## # ‚Ä¶ with 722 more rows We can delete the two not useful columns (i.e., trash1 and trash2) by using the select() and - (minus) symbol. predictions_tidy_v2 &lt;- predictions_tidy_v2 %&gt;% select(-trash1, -trash2) 6.14 Example of Plotting Now that we have our tidy dataframe (i.e., predictions_tidy_v2), we can plot temperatures by year: predictions_tidy_v2 %&gt;% ggplot(aes(x = Year, y = temperature)) + geom_point(aes(color = month)) 6.15 DATA CHALLENGE 01 Accept data challenge 01 assignment "],["data-wrangling.html", "Module 7 Data Wrangling 7.1 Load libraries 7.2 Read your data in 7.3 Summarise data 7.4 Tidy data 7.5 Transform Data 7.6 DATA CHALLENGE 02", " Module 7 Data Wrangling 7.1 Load libraries Load tidyverse using library() Our data for this module is an excel spreadsheet, so we need to install a new package to handle this type of data. install.packages(&quot;readxl&quot;) 7.2 Read your data in After readxl package installation is done: load readxl using library() check your working environment with getwd() and dir() load your data nfl_salary &lt;- read_excel(&quot;data/nfl_salary.xlsx&quot;) inspect your data with summary(), glimpse() and View() glimpse(nfl_salary) ## Rows: 800 ## Columns: 11 ## $ year &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2‚Ä¶ ## $ Cornerback &lt;dbl&gt; 11265916, 11000000, 10000000, 10000000, 10000000,‚Ä¶ ## $ `Defensive Lineman` &lt;dbl&gt; 17818000, 16200000, 12476000, 11904706, 11762782,‚Ä¶ ## $ Linebacker &lt;dbl&gt; 16420000, 15623000, 11825000, 10083333, 10020000,‚Ä¶ ## $ `Offensive Lineman` &lt;dbl&gt; 15960000, 12800000, 11767500, 10358200, 10000000,‚Ä¶ ## $ Quarterback &lt;dbl&gt; 17228125, 16000000, 14400000, 14100000, 13510000,‚Ä¶ ## $ `Running Back` &lt;dbl&gt; 12955000, 10873833, 9479000, 7700000, 7500000, 70‚Ä¶ ## $ Safety &lt;dbl&gt; 8871428, 8787500, 8282500, 8000000, 7804333, 7652‚Ä¶ ## $ `Special Teamer` &lt;dbl&gt; 4300000, 3725000, 3556176, 3500000, 3250000, 3225‚Ä¶ ## $ `Tight End` &lt;dbl&gt; 8734375, 8591000, 8290000, 7723333, 6974666, 6133‚Ä¶ ## $ `Wide Receiver` &lt;dbl&gt; 16250000, 14175000, 11424000, 11415000, 10800000,‚Ä¶ How many observations are there? What variables are there in the data? 7.3 Summarise data QUESTIONS: Have salaries for different NFL positions increased between 2011 and 2018? What positions pay more and less? Let‚Äôs summarise the mean salary for Quarterback by year. nfl_salary %&gt;% group_by(year) %&gt;% summarise(quarterback_mean_salary = mean(Quarterback, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 2 ## year quarterback_mean_salary ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 3376113. ## 2 2012 3496408. ## 3 2013 3450185. ## 4 2014 4234160. ## 5 2015 4225789. ## 6 2016 5499939. ## 7 2017 5329727. ## 8 2018 6593769. What would we do to add the mean salary for Cornerback? nfl_salary %&gt;% group_by(year) %&gt;% summarise(quarterback_mean_salary = mean(Quarterback, na.rm = TRUE), cornerback_mean_salary = mean(Cornerback, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 3 ## year quarterback_mean_salary cornerback_mean_salary ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 3376113. 3037766. ## 2 2012 3496408. 3132916. ## 3 2013 3450185. 2901798. ## 4 2014 4234160. 3038278. ## 5 2015 4225789. 3758543. ## 6 2016 5499939. 4201470. ## 7 2017 5329727. 4125692. ## 8 2018 6593769. 4659704. Let‚Äôs stop and think about how our data is organized. Is our data tidy? We have columns that mix two type of variables: categorical variable for position numeric variable for salary 7.4 Tidy data In order to make our data easier to work with, we need to make sure each column in our data represents just one variable. To do that for our nfl_salary dataframe, we need to pivot it. nfl_salary_tidy &lt;- nfl_salary %&gt;% pivot_longer(cols = -year, names_to = &quot;position&quot;, values_to = &quot;salary&quot;) Always inspect your new data frame. glimpse(nfl_salary_tidy) ## Rows: 8,000 ## Columns: 3 ## $ year &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, ‚Ä¶ ## $ position &lt;chr&gt; &quot;Cornerback&quot;, &quot;Defensive Lineman&quot;, &quot;Linebacker&quot;, &quot;Offensive ‚Ä¶ ## $ salary &lt;dbl&gt; 11265916, 17818000, 16420000, 15960000, 17228125, 12955000, ‚Ä¶ How many positions are there in the data? We can now do a count() with our categorical variable for position nfl_salary_tidy %&gt;% count(position) ## # A tibble: 10 x 2 ## position n ## &lt;chr&gt; &lt;int&gt; ## 1 Cornerback 800 ## 2 Defensive Lineman 800 ## 3 Linebacker 800 ## 4 Offensive Lineman 800 ## 5 Quarterback 800 ## 6 Running Back 800 ## 7 Safety 800 ## 8 Special Teamer 800 ## 9 Tight End 800 ## 10 Wide Receiver 800 We can add year to our group_by to check how many observations per position across year nfl_salary_tidy %&gt;% count(position, year) ## # A tibble: 80 x 3 ## position year n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Cornerback 2011 100 ## 2 Cornerback 2012 100 ## 3 Cornerback 2013 100 ## 4 Cornerback 2014 100 ## 5 Cornerback 2015 100 ## 6 Cornerback 2016 100 ## 7 Cornerback 2017 100 ## 8 Cornerback 2018 100 ## 9 Defensive Lineman 2011 100 ## 10 Defensive Lineman 2012 100 ## # ‚Ä¶ with 70 more rows Let‚Äôs check for NAs (i.e., missing data), we can do that by using is.na() and filter(). nfl_salary_tidy %&gt;% filter(is.na(salary)) %&gt;% count(position, year) ## # A tibble: 9 x 3 ## position year n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Quarterback 2011 3 ## 2 Quarterback 2012 12 ## 3 Quarterback 2013 7 ## 4 Quarterback 2014 11 ## 5 Quarterback 2015 3 ## 6 Quarterback 2016 5 ## 7 Quarterback 2017 3 ## 8 Quarterback 2018 11 ## 9 Special Teamer 2011 1 We can remove these rows from our data frame. nfl_salary_tidy_clean &lt;- nfl_salary_tidy %&gt;% filter(!is.na(salary)) Inspect your new data frame. glimpse(nfl_salary_tidy_clean) ## Rows: 7,944 ## Columns: 3 ## $ year &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, ‚Ä¶ ## $ position &lt;chr&gt; &quot;Cornerback&quot;, &quot;Defensive Lineman&quot;, &quot;Linebacker&quot;, &quot;Offensive ‚Ä¶ ## $ salary &lt;dbl&gt; 11265916, 17818000, 16420000, 15960000, 17228125, 12955000, ‚Ä¶ Now we can do our salary summarise() in a cleaner way. We are going to do a mean() of our numeric variable salary by year AND position. nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(mean_salary = mean(salary)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 80 x 3 ## # Groups: year [8] ## year position mean_salary ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2011 Cornerback 3037766. ## 2 2011 Defensive Lineman 4306995. ## 3 2011 Linebacker 4016045. ## 4 2011 Offensive Lineman 4662748. ## 5 2011 Quarterback 3376113. ## 6 2011 Running Back 1976341. ## 7 2011 Safety 2241891. ## 8 2011 Special Teamer 1244069. ## 9 2011 Tight End 1608100. ## 10 2011 Wide Receiver 2996590. ## # ‚Ä¶ with 70 more rows We can do the group_by both ways (first year and then position or vice-versa). nfl_salary_tidy_clean %&gt;% group_by(position, year) %&gt;% summarise(mean_salary = mean(salary)) %&gt;% arrange(mean_salary) ## `summarise()` regrouping output by &#39;position&#39; (override with `.groups` argument) ## # A tibble: 80 x 3 ## # Groups: position [10] ## position year mean_salary ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Special Teamer 2013 1235892. ## 2 Special Teamer 2011 1244069. ## 3 Special Teamer 2014 1264493. ## 4 Special Teamer 2012 1313043. ## 5 Special Teamer 2015 1348637. ## 6 Special Teamer 2016 1394443. ## 7 Special Teamer 2017 1459552. ## 8 Special Teamer 2018 1571447. ## 9 Tight End 2011 1608100. ## 10 Tight End 2012 1664520. ## # ‚Ä¶ with 70 more rows Add a - (minus) sign to the argument in arrange() to arrange your results by decreasing order of mean_salary. nfl_salary_tidy_clean %&gt;% group_by(position, year) %&gt;% summarise(mean_salary = mean(salary)) %&gt;% arrange(-mean_salary) ## `summarise()` regrouping output by &#39;position&#39; (override with `.groups` argument) ## # A tibble: 80 x 3 ## # Groups: position [10] ## position year mean_salary ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Offensive Lineman 2018 7522647. ## 2 Defensive Lineman 2018 7202360. ## 3 Quarterback 2018 6593769. ## 4 Offensive Lineman 2017 6370947. ## 5 Defensive Lineman 2017 6202601. ## 6 Wide Receiver 2018 5627721. ## 7 Quarterback 2016 5499939. ## 8 Offensive Lineman 2016 5410392. ## 9 Quarterback 2017 5329727. ## 10 Linebacker 2018 5293675. ## # ‚Ä¶ with 70 more rows We can also add arrange() to our code block. nfl_salary_tidy_clean %&gt;% group_by(position, year) %&gt;% summarise(mean_salary = mean(salary)) %&gt;% arrange() ## `summarise()` regrouping output by &#39;position&#39; (override with `.groups` argument) ## # A tibble: 80 x 3 ## # Groups: position [10] ## position year mean_salary ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cornerback 2011 3037766. ## 2 Cornerback 2012 3132916. ## 3 Cornerback 2013 2901798. ## 4 Cornerback 2014 3038278. ## 5 Cornerback 2015 3758543. ## 6 Cornerback 2016 4201470. ## 7 Cornerback 2017 4125692. ## 8 Cornerback 2018 4659704. ## 9 Defensive Lineman 2011 4306995. ## 10 Defensive Lineman 2012 4693730. ## # ‚Ä¶ with 70 more rows 7.4.1 Viz Demo We can also visualize our data using ggplot(). First we save our summary results in a new dataframe called nfl_salary_summary. nfl_salary_summary &lt;- nfl_salary_tidy_clean %&gt;% group_by(position, year) %&gt;% summarise(mean_salary = mean(salary)) %&gt;% arrange() ## `summarise()` regrouping output by &#39;position&#39; (override with `.groups` argument) Then we plot it. nfl_salary_summary %&gt;% ggplot(aes(x = year, y = mean_salary, color = position, group = position)) + geom_point() + geom_line() 7.5 Transform Data Now that our data is tidy, we can transform our data by adding new variables/columns to it. It seems some salaries for certain positions show a higher increase across the years than the salaries for other positions. In other words, the proportion of what position makes in relation to total money spent in salaries for each each. We can check this is true by creating a sum() of salaries for each year and a count of players using n(): nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(player_count = n(), total_per_position = sum(salary)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 80 x 4 ## # Groups: year [8] ## year position player_count total_per_position ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2011 Cornerback 100 303776605 ## 2 2011 Defensive Lineman 100 430699528 ## 3 2011 Linebacker 100 401604548 ## 4 2011 Offensive Lineman 100 466274753 ## 5 2011 Quarterback 97 327482939 ## 6 2011 Running Back 100 197634074 ## 7 2011 Safety 100 224189136 ## 8 2011 Special Teamer 99 123162874 ## 9 2011 Tight End 100 160810030 ## 10 2011 Wide Receiver 100 299659044 ## # ‚Ä¶ with 70 more rows We can then add mutate() to our code block to calculate sum() of all salaries per year. nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(player_count = n(), total_per_position = sum(salary)) %&gt;% mutate(total_per_year = sum(total_per_position)) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 80 x 5 ## # Groups: year [8] ## year position player_count total_per_position total_per_year ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 Cornerback 100 303776605 2935293531 ## 2 2011 Defensive Lineman 100 430699528 2935293531 ## 3 2011 Linebacker 100 401604548 2935293531 ## 4 2011 Offensive Lineman 100 466274753 2935293531 ## 5 2011 Quarterback 97 327482939 2935293531 ## 6 2011 Running Back 100 197634074 2935293531 ## 7 2011 Safety 100 224189136 2935293531 ## 8 2011 Special Teamer 99 123162874 2935293531 ## 9 2011 Tight End 100 160810030 2935293531 ## 10 2011 Wide Receiver 100 299659044 2935293531 ## # ‚Ä¶ with 70 more rows Now we can calculate the percentage cost of each position by the total salaries for each year, we can do that all in the same mutate(). nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(player_count = n(), total_per_position = sum(salary)) %&gt;% mutate(total_per_year = sum(total_per_position), percentage_cost = total_per_position/total_per_year) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 80 x 6 ## # Groups: year [8] ## year position player_count total_per_posit‚Ä¶ total_per_year percentage_cost ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 Cornerback 100 303776605 2935293531 0.103 ## 2 2011 Defensive‚Ä¶ 100 430699528 2935293531 0.147 ## 3 2011 Linebacker 100 401604548 2935293531 0.137 ## 4 2011 Offensive‚Ä¶ 100 466274753 2935293531 0.159 ## 5 2011 Quarterba‚Ä¶ 97 327482939 2935293531 0.112 ## 6 2011 Running B‚Ä¶ 100 197634074 2935293531 0.0673 ## 7 2011 Safety 100 224189136 2935293531 0.0764 ## 8 2011 Special T‚Ä¶ 99 123162874 2935293531 0.0420 ## 9 2011 Tight End 100 160810030 2935293531 0.0548 ## 10 2011 Wide Rece‚Ä¶ 100 299659044 2935293531 0.102 ## # ‚Ä¶ with 70 more rows Add arrange() to see higher percentages at the top. nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(player_count = n(), total_per_position = sum(salary)) %&gt;% mutate(total_per_year = sum(total_per_position), percentage_cost = total_per_position/total_per_year) %&gt;% arrange(-percentage_cost) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) ## # A tibble: 80 x 6 ## # Groups: year [8] ## year position player_count total_per_posit‚Ä¶ total_per_year percentage_cost ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018 Offensive‚Ä¶ 100 752264724 4557047519 0.165 ## 2 2014 Defensive‚Ä¶ 100 503535499 3154183189 0.160 ## 3 2011 Offensive‚Ä¶ 100 466274753 2935293531 0.159 ## 4 2017 Offensive‚Ä¶ 100 637094749 4027571325 0.158 ## 5 2018 Defensive‚Ä¶ 100 720236012 4557047519 0.158 ## 6 2013 Defensive‚Ä¶ 100 454787761 2920039442 0.156 ## 7 2014 Offensive‚Ä¶ 100 489885308 3154183189 0.155 ## 8 2013 Offensive‚Ä¶ 100 453489965 2920039442 0.155 ## 9 2012 Defensive‚Ä¶ 100 469373045 3032589536 0.155 ## 10 2017 Defensive‚Ä¶ 100 620260110 4027571325 0.154 ## # ‚Ä¶ with 70 more rows 7.5.1 Viz Demo We can also visualize our data using ggplot(). First we save our summary results in a new dataframe called nfl_salary_summary. nfl_salary_summary &lt;- nfl_salary_tidy_clean %&gt;% group_by(year, position) %&gt;% summarise(player_count = n(), total_per_position = sum(salary)) %&gt;% mutate(total_per_year = sum(total_per_position), percentage_cost = total_per_position/total_per_year) %&gt;% arrange(-percentage_cost) ## `summarise()` regrouping output by &#39;year&#39; (override with `.groups` argument) Then we plot it. nfl_salary_summary %&gt;% ggplot(aes(x = year, y = percentage_cost, color = position, group = position)) + geom_point() + geom_line() 7.6 DATA CHALLENGE 02 Accept data challenge 02 assignment "],["data-visualization.html", "Module 8 Data Visualization 8.1 The layered grammar of graphics 8.2 Load data 8.3 What to plot? 8.4 Aesthetic Mappings 8.5 geom_ (i.e., Geometric Objects) 8.6 More mappings with aes() 8.7 Facets 8.8 More Summarize 8.9 DATA CHALLENGE 03", " Module 8 Data Visualization 8.1 The layered grammar of graphics The package we will be using for plotting in this class is called ggplot2 which is part of tidyverse, and it uses as a principle the idea of layered grammar of graphics. That means you can add code to add or change your plot in layers, by using the + symbol. Let‚Äôs start with some data, so we can created different plots using different layers. 8.2 Load data Get data directly from tidy tuesday github. ## Rows: 32,833 ## Columns: 23 ## $ track_id &lt;chr&gt; &quot;6f807x0ima9a1j3VPbc7VN&quot;, &quot;0r7CVbZTWZgbTCYdf‚Ä¶ ## $ track_name &lt;chr&gt; &quot;I Don&#39;t Care (with Justin Bieber) - Loud Lu‚Ä¶ ## $ track_artist &lt;chr&gt; &quot;Ed Sheeran&quot;, &quot;Maroon 5&quot;, &quot;Zara Larsson&quot;, &quot;T‚Ä¶ ## $ track_popularity &lt;dbl&gt; 66, 67, 70, 60, 69, 67, 62, 69, 68, 67, 58, ‚Ä¶ ## $ track_album_id &lt;chr&gt; &quot;2oCs0DGTsRO98Gh5ZSl2Cx&quot;, &quot;63rPSO264uRjW1X5E‚Ä¶ ## $ track_album_name &lt;chr&gt; &quot;I Don&#39;t Care (with Justin Bieber) [Loud Lux‚Ä¶ ## $ track_album_release_date &lt;chr&gt; &quot;2019-06-14&quot;, &quot;2019-12-13&quot;, &quot;2019-07-05&quot;, &quot;2‚Ä¶ ## $ playlist_name &lt;chr&gt; &quot;Pop Remix&quot;, &quot;Pop Remix&quot;, &quot;Pop Remix&quot;, &quot;Pop ‚Ä¶ ## $ playlist_id &lt;chr&gt; &quot;37i9dQZF1DXcZDD7cfEKhW&quot;, &quot;37i9dQZF1DXcZDD7c‚Ä¶ ## $ playlist_genre &lt;chr&gt; &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;p‚Ä¶ ## $ playlist_subgenre &lt;chr&gt; &quot;dance pop&quot;, &quot;dance pop&quot;, &quot;dance pop&quot;, &quot;danc‚Ä¶ ## $ danceability &lt;dbl&gt; 0.748, 0.726, 0.675, 0.718, 0.650, 0.675, 0.‚Ä¶ ## $ energy &lt;dbl&gt; 0.916, 0.815, 0.931, 0.930, 0.833, 0.919, 0.‚Ä¶ ## $ key &lt;dbl&gt; 6, 11, 1, 7, 1, 8, 5, 4, 8, 2, 6, 8, 1, 5, 5‚Ä¶ ## $ loudness &lt;dbl&gt; -2.634, -4.969, -3.432, -3.778, -4.672, -5.3‚Ä¶ ## $ mode &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,‚Ä¶ ## $ speechiness &lt;dbl&gt; 0.0583, 0.0373, 0.0742, 0.1020, 0.0359, 0.12‚Ä¶ ## $ acousticness &lt;dbl&gt; 0.10200, 0.07240, 0.07940, 0.02870, 0.08030,‚Ä¶ ## $ instrumentalness &lt;dbl&gt; 0.00e+00, 4.21e-03, 2.33e-05, 9.43e-06, 0.00‚Ä¶ ## $ liveness &lt;dbl&gt; 0.0653, 0.3570, 0.1100, 0.2040, 0.0833, 0.14‚Ä¶ ## $ valence &lt;dbl&gt; 0.518, 0.693, 0.613, 0.277, 0.725, 0.585, 0.‚Ä¶ ## $ tempo &lt;dbl&gt; 122.036, 99.972, 124.008, 121.956, 123.976, ‚Ä¶ ## $ duration_ms &lt;dbl&gt; 194754, 162600, 176616, 169093, 189052, 1630‚Ä¶ EXERCISE What variables do we have in this data? What questions can you ask about this data? 8.3 What to plot? The first thing you need to do is define what you want to plot. If you‚Äôve never plotted data before, you might not be familiar with the different types of charts you can create. Here‚Äôs a few (can you tell what type of plot these are?): What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? What is the plot above called? What variable(s) are we plotting? What can we conclude based on this plot? 8.4 Aesthetic Mappings You map your aesthetics using the aes() function, which can be place inside of the ggplot() function. 8.4.1 One continuous variable You need to map at least one variable when you are plotting. Check the help for geom_histogram to see what kind of variable you can plot in a histogram. The variable track_popularity is continuous. spotify_songs %&gt;% ggplot(aes(x = track_popularity)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The variable release_year is also continuous. spotify_songs %&gt;% ggplot(aes(x = release_year)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 8.4.2 Two numeric variables Two-dimensional plots have two axis, x (horizontal) and y (vertical). spotify_songs %&gt;% ggplot(aes(x = release_year, y = track_popularity)) + geom_point() 8.5 geom_ (i.e., Geometric Objects) Functions such as geom_histogram(), geom_point(), and geom_col() are geometric objects and determine what type of plot R draws. You can also map other elements of your chart in addition to position (i.e., x and y), such as color, size, and shape. spotify_songs %&gt;% ggplot(aes(x = release_year, y = track_popularity, color = playlist_genre)) + geom_point() EXERCISE Check the help page for geom_point (enter ?geom_point in your console). Change geom_point() to the suggested variations in its help page. 8.6 More mappings with aes() In addition to color you can also add size and shape to aes(). spotify_songs %&gt;% ggplot(aes(x = release_year, y = track_popularity, color = playlist_subgenre, shape = playlist_genre)) + geom_point() The plot above is too messy, there‚Äôs too much information. You often need to transform your data before plotting it. EXERCISE Summarize mean track_popularity by release_year, playlist_genre and playlist_subgenre. Your summarized data frame should look something like this: ## `summarise()` regrouping output by &#39;release_year&#39;, &#39;playlist_genre&#39; (override with `.groups` argument) ## # A tibble: 883 x 4 ## # Groups: release_year, playlist_genre [302] ## release_year playlist_genre playlist_subgenre mean_popularity ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1957 r&amp;b urban contemporary 59 ## 2 1957 rock classic rock 1 ## 3 1958 rock classic rock 73 ## 4 1960 r&amp;b neo soul 13 ## 5 1960 r&amp;b urban contemporary 19 ## 6 1961 r&amp;b urban contemporary 47 ## 7 1962 r&amp;b urban contemporary 64 ## 8 1962 rock classic rock 64 ## 9 1963 r&amp;b neo soul 73 ## 10 1963 rock album rock 59 ## # ‚Ä¶ with 873 more rows Now you can plot your summarized data. spotify_summary %&gt;% ggplot(aes(x = release_year, y = mean_popularity, color = playlist_subgenre, shape = playlist_genre)) + geom_point() Still super messy. Shape is not really a good way to do this. Let‚Äôs try a bar plot. spotify_summary %&gt;% ggplot(aes(x = release_year, y = mean_popularity, fill = playlist_genre)) + geom_col(position = &quot;dodge&quot;) Not great either, too much going on. 8.7 Facets You can use the facet_wrap() function to split your plotting into several smaller plots, usually by a categorical variable. Let‚Äôs try the scatterplot first. spotify_summary %&gt;% ggplot(aes(x = release_year, y = mean_popularity, color = playlist_genre)) + geom_point() + facet_wrap(~playlist_subgenre) What about a bar plot? spotify_summary %&gt;% ggplot(aes(x = release_year, y = mean_popularity, fill = playlist_genre)) + geom_col(position = &quot;dodge&quot;) + facet_wrap(~playlist_subgenre) 8.8 More Summarize We can also plot categorical variables on the x axis. EXERCISE Summarize mean track_popularity by playlist_genre. Your summarized data frame should look something like this: ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 2 ## playlist_genre mean_popularity ## &lt;chr&gt; &lt;dbl&gt; ## 1 edm 34.8 ## 2 latin 47.0 ## 3 pop 47.7 ## 4 r&amp;b 41.2 ## 5 rap 43.2 ## 6 rock 41.7 Now plot a bar chart mapping x to playlist_genre, y to mean_popularity. Your plot should look something like this: 8.9 DATA CHALLENGE 03 Accept data challenge 03 assignment "],["data-visualization-ii.html", "Module 9 Data Visualization II 9.1 Data Viz by Artist 9.2 Data Viz by Album 9.3 DATA CHALLENGE 04", " Module 9 Data Visualization II We will continue working with the spotify data set we worked with last week. The objectives of this module are as follows: by the end of this module you will be able to ‚Ä¶ Explore a large data frame to decide what part of the data you want to focus on Create subsets of your original data frame Create summarizations of your data based on different variables Plot these summarizations ## Rows: 32,833 ## Columns: 25 ## $ track_id &lt;chr&gt; &quot;6f807x0ima9a1j3VPbc7VN&quot;, &quot;0r7CVbZTWZgbTCYdf‚Ä¶ ## $ track_name &lt;chr&gt; &quot;I Don&#39;t Care (with Justin Bieber) - Loud Lu‚Ä¶ ## $ track_artist &lt;chr&gt; &quot;Ed Sheeran&quot;, &quot;Maroon 5&quot;, &quot;Zara Larsson&quot;, &quot;T‚Ä¶ ## $ track_popularity &lt;dbl&gt; 66, 67, 70, 60, 69, 67, 62, 69, 68, 67, 58, ‚Ä¶ ## $ track_album_id &lt;chr&gt; &quot;2oCs0DGTsRO98Gh5ZSl2Cx&quot;, &quot;63rPSO264uRjW1X5E‚Ä¶ ## $ track_album_name &lt;chr&gt; &quot;I Don&#39;t Care (with Justin Bieber) [Loud Lux‚Ä¶ ## $ track_album_release_date &lt;chr&gt; &quot;2019-06-14&quot;, &quot;2019-12-13&quot;, &quot;2019-07-05&quot;, &quot;2‚Ä¶ ## $ playlist_name &lt;chr&gt; &quot;Pop Remix&quot;, &quot;Pop Remix&quot;, &quot;Pop Remix&quot;, &quot;Pop ‚Ä¶ ## $ playlist_id &lt;chr&gt; &quot;37i9dQZF1DXcZDD7cfEKhW&quot;, &quot;37i9dQZF1DXcZDD7c‚Ä¶ ## $ playlist_genre &lt;chr&gt; &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;p‚Ä¶ ## $ playlist_subgenre &lt;chr&gt; &quot;dance pop&quot;, &quot;dance pop&quot;, &quot;dance pop&quot;, &quot;danc‚Ä¶ ## $ danceability &lt;dbl&gt; 0.748, 0.726, 0.675, 0.718, 0.650, 0.675, 0.‚Ä¶ ## $ energy &lt;dbl&gt; 0.916, 0.815, 0.931, 0.930, 0.833, 0.919, 0.‚Ä¶ ## $ key &lt;dbl&gt; 6, 11, 1, 7, 1, 8, 5, 4, 8, 2, 6, 8, 1, 5, 5‚Ä¶ ## $ loudness &lt;dbl&gt; -2.634, -4.969, -3.432, -3.778, -4.672, -5.3‚Ä¶ ## $ mode &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,‚Ä¶ ## $ speechiness &lt;dbl&gt; 0.0583, 0.0373, 0.0742, 0.1020, 0.0359, 0.12‚Ä¶ ## $ acousticness &lt;dbl&gt; 0.10200, 0.07240, 0.07940, 0.02870, 0.08030,‚Ä¶ ## $ instrumentalness &lt;dbl&gt; 0.00e+00, 4.21e-03, 2.33e-05, 9.43e-06, 0.00‚Ä¶ ## $ liveness &lt;dbl&gt; 0.0653, 0.3570, 0.1100, 0.2040, 0.0833, 0.14‚Ä¶ ## $ valence &lt;dbl&gt; 0.518, 0.693, 0.613, 0.277, 0.725, 0.585, 0.‚Ä¶ ## $ tempo &lt;dbl&gt; 122.036, 99.972, 124.008, 121.956, 123.976, ‚Ä¶ ## $ duration_ms &lt;dbl&gt; 194754, 162600, 176616, 169093, 189052, 1630‚Ä¶ ## $ release_year &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 20‚Ä¶ ## $ decade &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20‚Ä¶ 9.1 Data Viz by Artist QUESTION: Which artist (from just a few) is most popular? Does that change across different decades? 9.1.1 Explore Artist Info Let‚Äôs check who the artists are in this data set. Check what the unique values are for the track_artist variable using select() and unique(). ## # A tibble: 10,693 x 1 ## track_artist ## &lt;chr&gt; ## 1 Ed Sheeran ## 2 Maroon 5 ## 3 Zara Larsson ## 4 The Chainsmokers ## 5 Lewis Capaldi ## 6 Katy Perry ## 7 Sam Feldt ## 8 Avicii ## 9 Shawn Mendes ## 10 Ellie Goulding ## # ‚Ä¶ with 10,683 more rows Who‚Äôs the artist with the most songs? Use count() and arrange() to find out. ## # A tibble: 10,693 x 2 ## track_artist n ## &lt;chr&gt; &lt;int&gt; ## 1 Martin Garrix 161 ## 2 Queen 136 ## 3 The Chainsmokers 123 ## 4 David Guetta 110 ## 5 Don Omar 102 ## 6 Drake 100 ## 7 Dimitri Vegas &amp; Like Mike 93 ## 8 Calvin Harris 91 ## 9 Hardwell 84 ## 10 Kygo 83 ## # ‚Ä¶ with 10,683 more rows What genre are these artists classified as? ## # A tibble: 13,175 x 3 ## track_artist playlist_genre n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Queen rock 134 ## 2 Martin Garrix edm 125 ## 3 Don Omar latin 100 ## 4 Dimitri Vegas &amp; Like Mike edm 79 ## 5 Guns N&#39; Roses rock 76 ## 6 Hardwell edm 76 ## 7 Logic rap 65 ## 8 Daddy Yankee latin 61 ## 9 David Guetta edm 60 ## 10 Wisin &amp; Yandel latin 60 ## # ‚Ä¶ with 13,165 more rows What can we conclude about artist tracks and playlist_genre? Let‚Äôs look at specific artist of our choosing. I‚Äôm looking at The Cranberries, The Beatles and Queen. What genres are their songs classfied as? ## # A tibble: 4 x 3 ## track_artist playlist_genre n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Queen pop 2 ## 2 Queen rock 134 ## 3 The Beatles rock 19 ## 4 The Cranberries rock 45 What are the two pop songs by Queen? Use filter() and select() to find out. ## # A tibble: 2 x 1 ## track_name ## &lt;chr&gt; ## 1 Don&#39;t Stop Me Now - 2011 Mix ## 2 Radio Ga Ga 9.1.2 Create new data frame with selected artists Create another data frame that is a subset of the original spotify_songs data frame to start visualizing info about the artists you chose. # filter original data frame to create new data frame with selected artists spotify_tc_tv_q &lt;- spotify_songs %&gt;% filter(track_artist %in% c(&#39;The Cranberries&#39;, &#39;The Beatles&#39;, &#39;Queen&#39;)) # inspect new data frame glimpse(spotify_tc_tv_q) ## Rows: 200 ## Columns: 25 ## $ track_id &lt;chr&gt; &quot;7hQJA50XrCWABAu5v6QZ4i&quot;, &quot;1lpFXKKckqVkyAN1l‚Ä¶ ## $ track_name &lt;chr&gt; &quot;Don&#39;t Stop Me Now - 2011 Mix&quot;, &quot;Radio Ga Ga‚Ä¶ ## $ track_artist &lt;chr&gt; &quot;Queen&quot;, &quot;Queen&quot;, &quot;The Beatles&quot;, &quot;The Cranbe‚Ä¶ ## $ track_popularity &lt;dbl&gt; 75, 3, 1, 43, 42, 44, 40, 40, 38, 37, 37, 38‚Ä¶ ## $ track_album_id &lt;chr&gt; &quot;21HMAUrbbYSj9NiPPlGumy&quot;, &quot;39MMaY4ampwjkSOFa‚Ä¶ ## $ track_album_name &lt;chr&gt; &quot;Jazz (Deluxe Remastered Version)&quot;, &quot;The Wor‚Ä¶ ## $ track_album_release_date &lt;chr&gt; &quot;1978-11-10&quot;, &quot;1984-02-27&quot;, &quot;1996-03-18&quot;, &quot;2‚Ä¶ ## $ playlist_name &lt;chr&gt; &quot;Dr. Q&#39;s Prescription Playlist\\U0001f48a&quot;, &quot;‚Ä¶ ## $ playlist_id &lt;chr&gt; &quot;6jAPdgY9XmxC9cgkXAVmVv&quot;, &quot;65HtIbyFkaQPflCa4‚Ä¶ ## $ playlist_genre &lt;chr&gt; &quot;pop&quot;, &quot;pop&quot;, &quot;rock&quot;, &quot;rock&quot;, &quot;rock&quot;, &quot;rock&quot;‚Ä¶ ## $ playlist_subgenre &lt;chr&gt; &quot;post-teen pop&quot;, &quot;electropop&quot;, &quot;album rock&quot;,‚Ä¶ ## $ danceability &lt;dbl&gt; 0.563, 0.762, 0.388, 0.529, 0.473, 0.437, 0.‚Ä¶ ## $ energy &lt;dbl&gt; 0.865, 0.414, 0.677, 0.845, 0.598, 0.785, 0.‚Ä¶ ## $ key &lt;dbl&gt; 5, 5, 8, 0, 6, 4, 0, 9, 7, 9, 7, 0, 0, 9, 9,‚Ä¶ ## $ loudness &lt;dbl&gt; -5.277, -12.036, -7.262, -5.432, -5.101, -4.‚Ä¶ ## $ mode &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,‚Ä¶ ## $ speechiness &lt;dbl&gt; 0.1600, 0.0379, 0.0301, 0.0294, 0.0268, 0.05‚Ä¶ ## $ acousticness &lt;dbl&gt; 0.047200, 0.173000, 0.052700, 0.000199, 0.03‚Ä¶ ## $ instrumentalness &lt;dbl&gt; 1.91e-04, 1.11e-04, 1.07e-02, 1.74e-01, 7.79‚Ä¶ ## $ liveness &lt;dbl&gt; 0.7700, 0.0942, 0.2210, 0.2270, 0.1250, 0.10‚Ä¶ ## $ valence &lt;dbl&gt; 0.6010, 0.7310, 0.4240, 0.5710, 0.0565, 0.48‚Ä¶ ## $ tempo &lt;dbl&gt; 156.271, 112.398, 175.818, 109.093, 93.022, ‚Ä¶ ## $ duration_ms &lt;dbl&gt; 209413, 349133, 234053, 256387, 239947, 2515‚Ä¶ ## $ release_year &lt;dbl&gt; 1978, 1984, 1996, 2019, 2019, 2019, 2019, 20‚Ä¶ ## $ decade &lt;dbl&gt; 1970, 1980, 1990, 2010, 2010, 2010, 2010, 20‚Ä¶ 9.1.3 Plotting Plot song count (x) by decade (y) the songs were release across track_artist (color). You need a count of track_artist and decade for this plot. spotify_tc_tv_q %&gt;% count(track_artist, decade) %&gt;% ggplot(aes(x = decade, y = n, color = track_artist)) + geom_point() To make tendencies clearer, we can add geom_line to our plot. We need a new aesthetics for the lines to connect the right points, called group. In this case, group takes the same variable as the color mapping. spotify_tc_tv_q %&gt;% count(track_artist, decade) %&gt;% ggplot(aes(x = decade, y = n, color = track_artist)) + geom_point() + geom_line(aes(group = track_artist)) From the plot above, what can we conclude about the selected artists? When did they start releasing songs? Let‚Äôs look at track_popularity by artist across decade. For this, we need group_by and summarise before we can build our plot. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(x = decade, y = mean_popularity, color = track_artist)) + geom_point() + geom_line(aes(group = track_artist)) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) How would this plot look like as a bar plot? spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(x = decade, y = mean_popularity, fill = track_artist)) + geom_col(position = &quot;dodge&quot;) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) Which chart do you think is easier to read? Why? We have multiple songs per artists, so we can include standard deviation in our summarise. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) ## # A tibble: 13 x 5 ## # Groups: track_artist [3] ## track_artist decade n mean_popularity sd_popularity ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Queen 1970 97 43.2 15.1 ## 2 Queen 1980 29 43.8 22.5 ## 3 Queen 1990 4 19.8 27.6 ## 4 Queen 2010 6 51.8 11.7 ## 5 The Beatles 1960 9 69.8 6.53 ## 6 The Beatles 1970 5 69.2 5.89 ## 7 The Beatles 1980 1 39 NA ## 8 The Beatles 1990 1 1 NA ## 9 The Beatles 2000 1 74 NA ## 10 The Beatles 2010 2 55.5 4.95 ## 11 The Cranberries 1990 31 52.5 13.3 ## 12 The Cranberries 2000 2 35.5 3.54 ## 13 The Cranberries 2010 12 37.6 12.3 NAs in our data frame is a problem. We can add mutate with replace_na to replace these NAs with zero. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0)) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) ## # A tibble: 13 x 5 ## # Groups: track_artist [3] ## track_artist decade n mean_popularity sd_popularity ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Queen 1970 97 43.2 15.1 ## 2 Queen 1980 29 43.8 22.5 ## 3 Queen 1990 4 19.8 27.6 ## 4 Queen 2010 6 51.8 11.7 ## 5 The Beatles 1960 9 69.8 6.53 ## 6 The Beatles 1970 5 69.2 5.89 ## 7 The Beatles 1980 1 39 0 ## 8 The Beatles 1990 1 1 0 ## 9 The Beatles 2000 1 74 0 ## 10 The Beatles 2010 2 55.5 4.95 ## 11 The Cranberries 1990 31 52.5 13.3 ## 12 The Cranberries 2000 2 35.5 3.54 ## 13 The Cranberries 2010 12 37.6 12.3 The data frame looks good, let‚Äôs add the plot code lines to the block of code above. This time, let‚Äôs do a bar chart faceted by track_artist. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0), lower = mean_popularity - sd_popularity, upper = mean_popularity + sd_popularity) %&gt;% ggplot(aes(x = decade, y = mean_popularity, fill = track_artist)) + geom_col() + facet_wrap(~track_artist) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) It looks the same as before. Let‚Äôs add geom_errorbar to it with ymin and ymax mappings. For that, we need to transform our data frame with mutate to calculate lower and upper variables, which represent the mean minus the standard deviation for the lower value of the range, and mean plus standard deviation for the upper value of the range. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0), lower = mean_popularity - sd_popularity, upper = mean_popularity + sd_popularity) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) ## # A tibble: 13 x 7 ## # Groups: track_artist [3] ## track_artist decade n mean_popularity sd_popularity lower upper ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Queen 1970 97 43.2 15.1 28.0 58.3 ## 2 Queen 1980 29 43.8 22.5 21.3 66.2 ## 3 Queen 1990 4 19.8 27.6 -7.83 47.3 ## 4 Queen 2010 6 51.8 11.7 40.2 63.5 ## 5 The Beatles 1960 9 69.8 6.53 63.2 76.3 ## 6 The Beatles 1970 5 69.2 5.89 63.3 75.1 ## 7 The Beatles 1980 1 39 0 39 39 ## 8 The Beatles 1990 1 1 0 1 1 ## 9 The Beatles 2000 1 74 0 74 74 ## 10 The Beatles 2010 2 55.5 4.95 50.6 60.4 ## 11 The Cranberries 1990 31 52.5 13.3 39.2 65.8 ## 12 The Cranberries 2000 2 35.5 3.54 32.0 39.0 ## 13 The Cranberries 2010 12 37.6 12.3 25.2 49.9 Now we can use geom_errorbar. spotify_tc_tv_q %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0), lower = mean_popularity - sd_popularity, upper = mean_popularity + sd_popularity) %&gt;% ggplot(aes(x = decade, y = mean_popularity, fill = track_artist)) + geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper)) + facet_wrap(~track_artist) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) We can do a similar chart but look at the 2010 decade only. spotify_tc_tv_q %&gt;% filter(decade == 2010) %&gt;% group_by(track_artist, decade) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0), lower = mean_popularity - sd_popularity, upper = mean_popularity + sd_popularity) %&gt;% ggplot(aes(x = track_artist, y = mean_popularity, fill = track_artist)) + geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper)) + facet_wrap(~decade) ## `summarise()` regrouping output by &#39;track_artist&#39; (override with `.groups` argument) We can also collapse decade, and just look at popularity overall. spotify_tc_tv_q %&gt;% group_by(track_artist) %&gt;% summarise(n = n(), mean_popularity = mean(track_popularity), sd_popularity = sd(track_popularity)) %&gt;% mutate(sd_popularity = replace_na(sd_popularity, 0), lower = mean_popularity - sd_popularity, upper = mean_popularity + sd_popularity) %&gt;% ggplot(aes(x = track_artist, y = mean_popularity, fill = track_artist)) + geom_col() + geom_errorbar(aes(ymin = lower, ymax = upper)) ## `summarise()` ungrouping output (override with `.groups` argument) 9.2 Data Viz by Album QUESTION: Which Drake album is the most popular? Let‚Äôs review the steps to answer our question: Create a new data frame that is a subset of our original data frame Summarize and transform our new data frame to create the variables we need to plot the info we need Try different plots until we find a plot that looks clear 9.2.1 Create new data frame We first filter our data frame by artist. # filter original data frame to create new data frame with selected artists spotify_drake &lt;- spotify_songs %&gt;% filter(track_artist == &#39;Drake&#39;) # inspect new data frame glimpse(spotify_drake) ## Rows: 100 ## Columns: 25 ## $ track_id &lt;chr&gt; &quot;76P07ei8drjrenqtvDbefy&quot;, &quot;1xznGGDReH1oQq0xz‚Ä¶ ## $ track_name &lt;chr&gt; &quot;Hotline Bling&quot;, &quot;One Dance&quot;, &quot;Too Good&quot;, &quot;B‚Ä¶ ## $ track_artist &lt;chr&gt; &quot;Drake&quot;, &quot;Drake&quot;, &quot;Drake&quot;, &quot;Drake&quot;, &quot;Drake&quot;,‚Ä¶ ## $ track_popularity &lt;dbl&gt; 0, 20, 12, 72, 12, 10, 83, 83, 86, 68, 15, 7‚Ä¶ ## $ track_album_id &lt;chr&gt; &quot;2e42oY2oFArkkTENT8UVXD&quot;, &quot;3hARKC8cinq3mZLLA‚Ä¶ ## $ track_album_name &lt;chr&gt; &quot;Views&quot;, &quot;Views&quot;, &quot;Views&quot;, &quot;Thank Me Later (‚Ä¶ ## $ track_album_release_date &lt;chr&gt; &quot;2016-05-06&quot;, &quot;2016-05-06&quot;, &quot;2016-05-06&quot;, &quot;2‚Ä¶ ## $ playlist_name &lt;chr&gt; &quot;BALLARE - ÿ±ŸÇÿµ&quot;, &quot;Electropop Hits 2017-2020‚Ä¶ ## $ playlist_id &lt;chr&gt; &quot;1CMvQ4Yr5DlYvYzI0Vc2UE&quot;, &quot;7kyvBmlc1uSqsTL0E‚Ä¶ ## $ playlist_genre &lt;chr&gt; &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;pop&quot;, &quot;p‚Ä¶ ## $ playlist_subgenre &lt;chr&gt; &quot;post-teen pop&quot;, &quot;electropop&quot;, &quot;electropop&quot;,‚Ä¶ ## $ danceability &lt;dbl&gt; 0.905, 0.791, 0.804, 0.431, 0.771, 0.893, 0.‚Ä¶ ## $ energy &lt;dbl&gt; 0.617, 0.619, 0.648, 0.894, 0.629, 0.639, 0.‚Ä¶ ## $ key &lt;dbl&gt; 2, 1, 7, 5, 1, 2, 1, 1, 7, 1, 11, 10, 2, 1, ‚Ä¶ ## $ loudness &lt;dbl&gt; -8.039, -5.886, -7.805, -2.673, -5.790, -7.8‚Ä¶ ## $ mode &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,‚Ä¶ ## $ speechiness &lt;dbl&gt; 0.0596, 0.0532, 0.1170, 0.3300, 0.0511, 0.05‚Ä¶ ## $ acousticness &lt;dbl&gt; 0.00287, 0.00784, 0.05730, 0.09510, 0.00802,‚Ä¶ ## $ instrumentalness &lt;dbl&gt; 4.40e-04, 4.23e-03, 3.49e-05, 0.00e+00, 2.52‚Ä¶ ## $ liveness &lt;dbl&gt; 0.0484, 0.3510, 0.1020, 0.1880, 0.3560, 0.03‚Ä¶ ## $ valence &lt;dbl&gt; 0.572, 0.371, 0.392, 0.604, 0.362, 0.579, 0.‚Ä¶ ## $ tempo &lt;dbl&gt; 134.972, 103.989, 117.983, 162.193, 103.918,‚Ä¶ ## $ duration_ms &lt;dbl&gt; 267187, 173987, 263373, 258760, 173975, 2670‚Ä¶ ## $ release_year &lt;dbl&gt; 2016, 2016, 2016, 2010, 2016, 2015, 2016, 20‚Ä¶ ## $ decade &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20‚Ä¶ What albums are there in this new data frame? spotify_drake %&gt;% count(track_album_name) %&gt;% arrange(-n) ## # A tibble: 27 x 2 ## track_album_name n ## &lt;chr&gt; &lt;int&gt; ## 1 Views 23 ## 2 Scorpion 16 ## 3 More Life 9 ## 4 The Best In The World Pack 7 ## 5 Take Care (Deluxe) 5 ## 6 What A Time To Be Alive 5 ## 7 If You&#39;re Reading This It&#39;s Too Late 4 ## 8 Care Package 3 ## 9 Hotline Bling 3 ## 10 Top Boy (A Selection of Music Inspired by the Series) 3 ## # ‚Ä¶ with 17 more rows 9.2.2 Summarize data Now we summarize our data for mean popularity per album. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 27 x 2 ## track_album_name mean_popularity ## &lt;chr&gt; &lt;dbl&gt; ## 1 0 To 100 / The Catch Up 5 ## 2 Back To Back 69 ## 3 Behind Barz (Bonus) 74 ## 4 Care Package 61.3 ## 5 Fake Love 6 ## 6 Forever 2 ## 7 Hold On, We&#39;re Going Home 1 ## 8 Hotline Bling 9.67 ## 9 If You&#39;re Reading This It&#39;s Too Late 18 ## 10 More Life 47.9 ## # ‚Ä¶ with 17 more rows 9.2.3 Plot summarized data We now add the ggplot code lines to our summarized data frame. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(y = track_album_name, x = mean_popularity)) + geom_col() ## `summarise()` ungrouping output (override with `.groups` argument) Let‚Äôs order the songs by mean_popularity. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(y = reorder(track_album_name, mean_popularity), x = mean_popularity)) + geom_col() ## `summarise()` ungrouping output (override with `.groups` argument) We can add labels to the bars, with the mean_popularity for each album using geom_label. A new mapping is needed for label, which is the same as the x mapping in this case. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(y = reorder(track_album_name, mean_popularity), x = mean_popularity)) + geom_col() + geom_label(aes(label = mean_popularity)) ## `summarise()` ungrouping output (override with `.groups` argument) We need to clean up the means. We can do that using format. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(y = reorder(track_album_name, mean_popularity), x = mean_popularity)) + geom_col() + geom_label(aes(label = format(mean_popularity, digits = 1))) ## `summarise()` ungrouping output (override with `.groups` argument) We can clean up our chart even more. spotify_drake %&gt;% group_by(track_album_name) %&gt;% summarise(mean_popularity = mean(track_popularity)) %&gt;% ggplot(aes(y = reorder(track_album_name, mean_popularity), x = mean_popularity)) + geom_col() + geom_label(aes(label = format(mean_popularity, digits = 1))) + xlab(&quot;mean popularity&quot;) + ylab(&quot;&quot;) + theme_bw() + ggtitle(&quot;Albums by Drake&quot;) ## `summarise()` ungrouping output (override with `.groups` argument) 9.3 DATA CHALLENGE 04 Accept data challenge 04 assignment "],["data-case-study-1.html", "Module 10 Data Case Study 1 10.1 Data Questions 10.2 Data Exploration 10.3 Calculate height and weight relationship 10.4 Plot kilos per centimeter across years 10.5 Plot weight over time 10.6 Plot height over time 10.7 Data Filtering 10.8 Plot height and weight across countries over the years 10.9 Solving the problem of team name changes over time 10.10 DATA CHALLENGE 05", " Module 10 Data Case Study 1 This is our first case study of the semester. The goal of each case study is to consolidate the content we‚Äôve covered so far in class, which at this point includes: Load libraries using library() Read data using read_csv() or read_excel() Inspect data using summary() or glimpse() and/or View() Review your data question (what variables do you need to answer your question?) Explore data using the following functions: count() group_by() + summarise() Plot data using the functions above and: filter() mutate() gglot(aes()) + geom_... # load library library(tidyverse) # read data in olympic_events &lt;- read_csv(&quot;data/olympic_history_athlete_events.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## Name = col_character(), ## Sex = col_character(), ## Age = col_double(), ## Height = col_double(), ## Weight = col_double(), ## Team = col_character(), ## NOC = col_character(), ## Games = col_character(), ## Year = col_double(), ## Season = col_character(), ## City = col_character(), ## Sport = col_character(), ## Event = col_character(), ## Medal = col_character() ## ) olympic_noc_regions &lt;- read_csv(&quot;data/olympic_history_noc_regions.csv&quot;) ## Parsed with column specification: ## cols( ## NOC = col_character(), ## region = col_character(), ## notes = col_character() ## ) # inspect data glimpse(olympic_events) ## Rows: 271,116 ## Columns: 15 ## $ ID &lt;dbl&gt; 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7,‚Ä¶ ## $ Name &lt;chr&gt; &quot;A Dijiang&quot;, &quot;A Lamusi&quot;, &quot;Gunnar Nielsen Aaby&quot;, &quot;Edgar Lindena‚Ä¶ ## $ Sex &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M‚Ä¶ ## $ Age &lt;dbl&gt; 24, 23, 24, 34, 21, 21, 25, 25, 27, 27, 31, 31, 31, 31, 33, 33‚Ä¶ ## $ Height &lt;dbl&gt; 180, 170, NA, NA, 185, 185, 185, 185, 185, 185, 188, 188, 188,‚Ä¶ ## $ Weight &lt;dbl&gt; 80, 60, NA, NA, 82, 82, 82, 82, 82, 82, 75, 75, 75, 75, 75, 75‚Ä¶ ## $ Team &lt;chr&gt; &quot;China&quot;, &quot;China&quot;, &quot;Denmark&quot;, &quot;Denmark/Sweden&quot;, &quot;Netherlands&quot;, ‚Ä¶ ## $ NOC &lt;chr&gt; &quot;CHN&quot;, &quot;CHN&quot;, &quot;DEN&quot;, &quot;DEN&quot;, &quot;NED&quot;, &quot;NED&quot;, &quot;NED&quot;, &quot;NED&quot;, &quot;NED&quot;,‚Ä¶ ## $ Games &lt;chr&gt; &quot;1992 Summer&quot;, &quot;2012 Summer&quot;, &quot;1920 Summer&quot;, &quot;1900 Summer&quot;, &quot;1‚Ä¶ ## $ Year &lt;dbl&gt; 1992, 2012, 1920, 1900, 1988, 1988, 1992, 1992, 1994, 1994, 19‚Ä¶ ## $ Season &lt;chr&gt; &quot;Summer&quot;, &quot;Summer&quot;, &quot;Summer&quot;, &quot;Summer&quot;, &quot;Winter&quot;, &quot;Winter&quot;, &quot;W‚Ä¶ ## $ City &lt;chr&gt; &quot;Barcelona&quot;, &quot;London&quot;, &quot;Antwerpen&quot;, &quot;Paris&quot;, &quot;Calgary&quot;, &quot;Calga‚Ä¶ ## $ Sport &lt;chr&gt; &quot;Basketball&quot;, &quot;Judo&quot;, &quot;Football&quot;, &quot;Tug-Of-War&quot;, &quot;Speed Skating‚Ä¶ ## $ Event &lt;chr&gt; &quot;Basketball Men&#39;s Basketball&quot;, &quot;Judo Men&#39;s Extra-Lightweight&quot;,‚Ä¶ ## $ Medal &lt;chr&gt; NA, NA, NA, &quot;Gold&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶ glimpse(olympic_noc_regions) ## Rows: 230 ## Columns: 3 ## $ NOC &lt;chr&gt; &quot;AFG&quot;, &quot;AHO&quot;, &quot;ALB&quot;, &quot;ALG&quot;, &quot;AND&quot;, &quot;ANG&quot;, &quot;ANT&quot;, &quot;ANZ&quot;, &quot;ARG&quot;,‚Ä¶ ## $ region &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Curacao&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Andorra&quot;, &quot;An‚Ä¶ ## $ notes &lt;chr&gt; NA, &quot;Netherlands Antilles&quot;, NA, NA, NA, NA, &quot;Antigua and Barbu‚Ä¶ 10.1 Data Questions Has athlete height and weight changed over time overall? Has height and weight changed over time for the top 5 countries with the most medals? 10.2 Data Exploration Suggestions of data exploration: What type of olympics? What years? How many athletes per game? ## # A tibble: 51 x 2 ## Games n ## &lt;chr&gt; &lt;int&gt; ## 1 1896 Summer 380 ## 2 1900 Summer 1936 ## 3 1904 Summer 1301 ## 4 1906 Summer 1733 ## 5 1908 Summer 3101 ## 6 1912 Summer 4040 ## 7 1920 Summer 4292 ## 8 1924 Summer 5233 ## 9 1924 Winter 460 ## 10 1928 Summer 4992 ## # ‚Ä¶ with 41 more rows This data set is huge, the plot below will take a while to process: What are the top countries for medal count? ## # A tibble: 783 x 3 ## Team Medal n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 United States Gold 2474 ## 2 United States Silver 1512 ## 3 United States Bronze 1233 ## 4 Soviet Union Gold 1058 ## 5 Soviet Union Silver 716 ## 6 Germany Gold 679 ## 7 Germany Bronze 678 ## 8 Soviet Union Bronze 677 ## 9 Germany Silver 627 ## 10 Great Britain Silver 582 ## # ‚Ä¶ with 773 more rows 10.3 Calculate height and weight relationship It‚Äôs difficult to plot three numeric variables (in this case we want to plot weight, height, across different years) with so many data points. So I‚Äôll create a new variable in my data frame with is the relationship between height and weight (i.e., kg per cm). ## # A tibble: 271,116 x 3 ## Weight Height kg_per_cm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 80 180 0.444 ## 2 60 170 0.353 ## 3 NA NA NA ## 4 NA NA NA ## 5 82 185 0.443 ## 6 82 185 0.443 ## 7 82 185 0.443 ## 8 82 185 0.443 ## 9 82 185 0.443 ## 10 82 185 0.443 ## # ‚Ä¶ with 271,106 more rows 10.4 Plot kilos per centimeter across years Now that we have this new variable, which is kilos per centimeters, we can plot this variable across years. Let‚Äôs see if summarised data (i.e., mean of kg_per_cm across year), makes more sense. Let‚Äôs split by Summer vs. Winter olympics. 10.5 Plot weight over time 10.6 Plot height over time 10.7 Data Filtering Get only top 5 countries for highest medal count ## # A tibble: 5 x 2 ## Team n ## &lt;chr&gt; &lt;int&gt; ## 1 United States 5219 ## 2 Soviet Union 2451 ## 3 Germany 1984 ## 4 Great Britain 1673 ## 5 France 1550 Create a list of these countries to filter the largest data set to create a smaller data frame. ## Rows: 56,100 ## Columns: 16 ## $ ID &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 34, 52, 56,‚Ä¶ ## $ Name &lt;chr&gt; &quot;Per Knut Aaland&quot;, &quot;Per Knut Aaland&quot;, &quot;Per Knut Aaland&quot;, &quot;P‚Ä¶ ## $ Sex &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;,‚Ä¶ ## $ Age &lt;dbl&gt; 31, 31, 31, 31, 33, 33, 33, 33, 31, 31, 31, 31, 33, 33, 33,‚Ä¶ ## $ Height &lt;dbl&gt; 188, 188, 188, 188, 188, 188, 188, 188, 183, 183, 183, 183,‚Ä¶ ## $ Weight &lt;dbl&gt; 75, 75, 75, 75, 75, 75, 75, 75, 72, 72, 72, 72, 72, 72, 72,‚Ä¶ ## $ Team &lt;chr&gt; &quot;United States&quot;, &quot;United States&quot;, &quot;United States&quot;, &quot;United ‚Ä¶ ## $ NOC &lt;chr&gt; &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;US‚Ä¶ ## $ Games &lt;chr&gt; &quot;1992 Winter&quot;, &quot;1992 Winter&quot;, &quot;1992 Winter&quot;, &quot;1992 Winter&quot;,‚Ä¶ ## $ Year &lt;dbl&gt; 1992, 1992, 1992, 1992, 1994, 1994, 1994, 1994, 1992, 1992,‚Ä¶ ## $ Season &lt;chr&gt; &quot;Winter&quot;, &quot;Winter&quot;, &quot;Winter&quot;, &quot;Winter&quot;, &quot;Winter&quot;, &quot;Winter&quot;,‚Ä¶ ## $ City &lt;chr&gt; &quot;Albertville&quot;, &quot;Albertville&quot;, &quot;Albertville&quot;, &quot;Albertville&quot;,‚Ä¶ ## $ Sport &lt;chr&gt; &quot;Cross Country Skiing&quot;, &quot;Cross Country Skiing&quot;, &quot;Cross Coun‚Ä¶ ## $ Event &lt;chr&gt; &quot;Cross Country Skiing Men&#39;s 10 kilometres&quot;, &quot;Cross Country ‚Ä¶ ## $ Medal &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶ ## $ kg_per_cm &lt;dbl&gt; 0.3989362, 0.3989362, 0.3989362, 0.3989362, 0.3989362, 0.39‚Ä¶ 10.8 Plot height and weight across countries over the years 10.9 Solving the problem of team name changes over time We can use name of Olympic committee (NOC) to standardized Team to country olympic_events %&gt;% count(NOC, Team) ## # A tibble: 1,231 x 3 ## NOC Team n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 AFG Afghanistan 126 ## 2 AHO Netherlands Antilles 79 ## 3 ALB Albania 70 ## 4 ALG Algeria 551 ## 5 AND Andorra 169 ## 6 ANG Angola 267 ## 7 ANT Antigua and Barbuda 133 ## 8 ANZ Australasia 77 ## 9 ANZ Sydney Rowing Club 9 ## 10 ARG Acturus 2 ## # ‚Ä¶ with 1,221 more rows The olympic_noc_regions data frame can help with that process. head(olympic_noc_regions) ## # A tibble: 6 x 3 ## NOC region notes ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AFG Afghanistan &lt;NA&gt; ## 2 AHO Curacao Netherlands Antilles ## 3 ALB Albania &lt;NA&gt; ## 4 ALG Algeria &lt;NA&gt; ## 5 AND Andorra &lt;NA&gt; ## 6 ANG Angola &lt;NA&gt; We can add region to our olympic_events data frame by joining the olympic_events data frame with the olympic_noc_regions data frame by the NOC column. For that we will use the left_join() function. olympic_events &lt;- left_join(olympic_events, olympic_noc_regions) ## Joining, by = &quot;NOC&quot; olympic_events %&gt;% count(NOC, region) ## # A tibble: 230 x 3 ## NOC region n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 AFG Afghanistan 126 ## 2 AHO Curacao 79 ## 3 ALB Albania 70 ## 4 ALG Algeria 551 ## 5 AND Andorra 169 ## 6 ANG Angola 267 ## 7 ANT Antigua 133 ## 8 ANZ Australia 86 ## 9 ARG Argentina 3297 ## 10 ARM Armenia 221 ## # ‚Ä¶ with 220 more rows Let‚Äôs check what we have for Russia now. olympic_events %&gt;% filter(region == &quot;Russia&quot;) %&gt;% count(NOC, region) ## # A tibble: 3 x 3 ## NOC region n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 EUN Russia 864 ## 2 RUS Russia 5143 ## 3 URS Russia 5685 We can now calculate medals per region instead of team name. Get only top 5 countries for highest medal count ## # A tibble: 5 x 2 ## region n ## &lt;chr&gt; &lt;int&gt; ## 1 USA 5637 ## 2 Russia 3947 ## 3 Germany 3756 ## 4 UK 2068 ## 5 France 1777 10.10 DATA CHALLENGE 05 Accept data challenge 05 assignment "],["data-case-study-2.html", "Module 11 Data Case Study 2 11.1 Reading data from a URL 11.2 Cleaning up column names 11.3 Manipulating Dates 11.4 Extra Data libraries 11.5 Exploring Data 11.6 DATA CHALLENGE 06", " Module 11 Data Case Study 2 We‚Äôve been working mainly with the tidyverse library, but today we will work with a few different libraries. The packages janitor and lubridate are very useful. Maybe sure you have these installed (use install.packages) before you load these libraries. library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test library(lubridate) library(tidyverse) 11.1 Reading data from a URL Today we are working with a large data set on global land temperatures. I added the csv file to github because it‚Äôs a very large file. You can use read_csv to read the file directly from github global_temperatures &lt;- read_csv(&quot;https://raw.githubusercontent.com/esoc214/fall2020_002_class_scripts/main/data/GlobalLandTemperaturesByCountry.csv&quot;) ## Parsed with column specification: ## cols( ## dt = col_date(format = &quot;&quot;), ## AverageTemperature = col_double(), ## AverageTemperatureUncertainty = col_double(), ## Country = col_character() ## ) # inspect data glimpse(global_temperatures) ## Rows: 577,462 ## Columns: 4 ## $ dt &lt;date&gt; 1743-11-01, 1743-12-01, 1744-01-01, 17‚Ä¶ ## $ AverageTemperature &lt;dbl&gt; 4.384, NA, NA, NA, NA, 1.530, 6.702, 11‚Ä¶ ## $ AverageTemperatureUncertainty &lt;dbl&gt; 2.294, NA, NA, NA, NA, 4.680, 1.789, 1.‚Ä¶ ## $ Country &lt;chr&gt; &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öl‚Ä¶ 11.2 Cleaning up column names Column names from different data sets usually have a number of different casings (Camel, Pascal, Snake, Kebab Case, etc.). I like to use clean_names() to standardize column names to snake_case. global_temperatures &lt;- global_temperatures %&gt;% clean_names() # inspect data glimpse(global_temperatures) ## Rows: 577,462 ## Columns: 4 ## $ dt &lt;date&gt; 1743-11-01, 1743-12-01, 1744-01-01, ‚Ä¶ ## $ average_temperature &lt;dbl&gt; 4.384, NA, NA, NA, NA, 1.530, 6.702, ‚Ä¶ ## $ average_temperature_uncertainty &lt;dbl&gt; 2.294, NA, NA, NA, NA, 4.680, 1.789, ‚Ä¶ ## $ country &lt;chr&gt; &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;‚Ä¶ 11.3 Manipulating Dates Now, let‚Äôs turn our attention to the dt column, which is a date. class(global_temperatures$dt) ## [1] &quot;Date&quot; There‚Äôs a number of functions we can run on a Date variable. # extract year from dt variable in global_temperatures year(global_temperatures$dt)[c(1:10)] ## [1] 1743 1743 1744 1744 1744 1744 1744 1744 1744 1744 # extract month from dt variable in global_temperatures month(global_temperatures$dt)[c(1:10)] ## [1] 11 12 1 2 3 4 5 6 7 8 month(global_temperatures$dt, label = TRUE)[c(1:10)] ## [1] Nov Dec Jan Feb Mar Apr May Jun Jul Aug ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec # extract week from dt variable in global_temperatures week(global_temperatures$dt)[c(1:10)] ## [1] 44 48 1 5 9 14 18 22 27 31 We can create new columns in our data frame with year, month, and week extracted from dt in our data frame by using mutate. global_temperatures %&gt;% mutate(year = year(dt), month = month(dt), decade = year - (year %% 10)) ## # A tibble: 577,462 x 7 ## dt average_temperat‚Ä¶ average_temperature_‚Ä¶ country year month decade ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1743-11-01 4.38 2.29 √Öland 1743 11 1740 ## 2 1743-12-01 NA NA √Öland 1743 12 1740 ## 3 1744-01-01 NA NA √Öland 1744 1 1740 ## 4 1744-02-01 NA NA √Öland 1744 2 1740 ## 5 1744-03-01 NA NA √Öland 1744 3 1740 ## 6 1744-04-01 1.53 4.68 √Öland 1744 4 1740 ## 7 1744-05-01 6.70 1.79 √Öland 1744 5 1740 ## 8 1744-06-01 11.6 1.58 √Öland 1744 6 1740 ## 9 1744-07-01 15.3 1.41 √Öland 1744 7 1740 ## 10 1744-08-01 NA NA √Öland 1744 8 1740 ## # ‚Ä¶ with 577,452 more rows Instead of just printing to the console, let‚Äôs change the original data frame. global_temperatures &lt;- global_temperatures %&gt;% mutate(year = year(dt), month = month(dt), decade = gsub(&quot;([12][0-9][0-9])[0-9]&quot;, &quot;\\\\10&quot;, year)) # inspect changes glimpse(global_temperatures) ## Rows: 577,462 ## Columns: 7 ## $ dt &lt;date&gt; 1743-11-01, 1743-12-01, 1744-01-01, ‚Ä¶ ## $ average_temperature &lt;dbl&gt; 4.384, NA, NA, NA, NA, 1.530, 6.702, ‚Ä¶ ## $ average_temperature_uncertainty &lt;dbl&gt; 2.294, NA, NA, NA, NA, 4.680, 1.789, ‚Ä¶ ## $ country &lt;chr&gt; &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;‚Ä¶ ## $ year &lt;dbl&gt; 1743, 1743, 1744, 1744, 1744, 1744, 1‚Ä¶ ## $ month &lt;dbl&gt; 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶ ## $ decade &lt;chr&gt; &quot;1740&quot;, &quot;1740&quot;, &quot;1740&quot;, &quot;1740&quot;, &quot;1740‚Ä¶ 11.4 Extra Data libraries If you need some hierarchical information that is not present in your data frame, such as continent based on country, check if there is a package that will do that for you. In our case, we will use the countrycode package. # make sure you install this library library(countrycode) Let‚Äôs look at our country variable. # what does country look like in our data? global_temperatures %&gt;% count(country) ## # A tibble: 243 x 2 ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 2106 ## 2 Africa 1965 ## 3 √Öland 3239 ## 4 Albania 3239 ## 5 Algeria 2721 ## 6 American Samoa 1761 ## 7 Andorra 3239 ## 8 Angola 1878 ## 9 Anguilla 2277 ## 10 Antarctica 764 ## # ‚Ä¶ with 233 more rows We use the function countrycode() to get continent from a country name. # create new column called continent global_temperatures &lt;- global_temperatures %&gt;% mutate(continent = countrycode(sourcevar = country, origin = &quot;country.name&quot;, destination = &quot;continent&quot;)) ## Warning: Problem with `mutate()` input `continent`. ## ‚Ñπ Some values were not matched unambiguously: Africa, Antarctica, Asia, Baker Island, Europe, French Southern And Antarctic Lands, Heard Island And Mcdonald Islands, Kingman Reef, North America, Oceania, Palmyra Atoll, Saint Martin, South America, South Georgia And The South Sandwich Isla, Virgin Islands ## ## ‚Ñπ Input `continent` is `countrycode(sourcevar = country, origin = &quot;country.name&quot;, destination = &quot;continent&quot;)`. ## Warning in countrycode(sourcevar = country, origin = &quot;country.name&quot;, destination = &quot;continent&quot;): Some values were not matched unambiguously: Africa, Antarctica, Asia, Baker Island, Europe, French Southern And Antarctic Lands, Heard Island And Mcdonald Islands, Kingman Reef, North America, Oceania, Palmyra Atoll, Saint Martin, South America, South Georgia And The South Sandwich Isla, Virgin Islands # inspect data glimpse(global_temperatures) ## Rows: 577,462 ## Columns: 8 ## $ dt &lt;date&gt; 1743-11-01, 1743-12-01, 1744-01-01, ‚Ä¶ ## $ average_temperature &lt;dbl&gt; 4.384, NA, NA, NA, NA, 1.530, 6.702, ‚Ä¶ ## $ average_temperature_uncertainty &lt;dbl&gt; 2.294, NA, NA, NA, NA, 4.680, 1.789, ‚Ä¶ ## $ country &lt;chr&gt; &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;√Öland&quot;, &quot;‚Ä¶ ## $ year &lt;dbl&gt; 1743, 1743, 1744, 1744, 1744, 1744, 1‚Ä¶ ## $ month &lt;dbl&gt; 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10‚Ä¶ ## $ decade &lt;chr&gt; &quot;1740&quot;, &quot;1740&quot;, &quot;1740&quot;, &quot;1740&quot;, &quot;1740‚Ä¶ ## $ continent &lt;chr&gt; &quot;Europe&quot;, &quot;Europe&quot;, &quot;Europe&quot;, &quot;Europe‚Ä¶ Always check your data. Let‚Äôs look at continent more closely, especially since we got a warning message. # why some continents were not assigned global_temperatures %&gt;% filter(is.na(continent)) %&gt;% distinct(country) ## # A tibble: 15 x 1 ## country ## &lt;chr&gt; ## 1 Africa ## 2 Antarctica ## 3 Asia ## 4 Baker Island ## 5 Europe ## 6 French Southern And Antarctic Lands ## 7 Heard Island And Mcdonald Islands ## 8 Kingman Reef ## 9 North America ## 10 Oceania ## 11 Palmyra Atoll ## 12 Saint Martin ## 13 South America ## 14 South Georgia And The South Sandwich Isla ## 15 Virgin Islands It seems we can eliminate the rows with NA for continent. # keep only continents that are not NA global_temp_continents &lt;- global_temperatures %&gt;% filter(!is.na(continent)) 11.5 Exploring Data Now that our data is clean and transformed, what‚Äôs our data question? The data frame is still very large, so some summarization would be helpful. global_temp_continents %&gt;% group_by(decade, continent) %&gt;% summarise(mean_temp = mean(average_temperature, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;decade&#39; (override with `.groups` argument) ## # A tibble: 128 x 3 ## # Groups: decade [28] ## decade continent mean_temp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1740 Americas 2.62 ## 2 1740 Europe 6.98 ## 3 1750 Africa 19.8 ## 4 1750 Americas 7.30 ## 5 1750 Europe 8.53 ## 6 1760 Africa 19.8 ## 7 1760 Americas 6.44 ## 8 1760 Europe 8.31 ## 9 1770 Africa 20.1 ## 10 1770 Americas 6.61 ## # ‚Ä¶ with 118 more rows It‚Äôs still a long data frame, plotting helps here. global_temp_continents %&gt;% group_by(decade, continent) %&gt;% summarise(mean_temp = mean(average_temperature, na.rm = TRUE)) %&gt;% ggplot(aes(x = decade, y = mean_temp, color = continent)) + geom_point() ## `summarise()` regrouping output by &#39;decade&#39; (override with `.groups` argument) Looking at the plot above, how reliable do you think these temperatures are? Why? Let‚Äôs look at Europe only. europe_temps &lt;- global_temp_continents %&gt;% filter(continent == &quot;Europe&quot;) europe_temps %&gt;% group_by(decade) %&gt;% summarise(mean_temp = mean(average_temperature, na.rm = TRUE)) %&gt;% ggplot(aes(x = decade, y = mean_temp)) + geom_point() ## `summarise()` ungrouping output (override with `.groups` argument) Now that we have filtered by continent, we can look at year instead of decade, because you have less information to plot (just one continent). europe_temps %&gt;% group_by(year) %&gt;% summarise(mean_temp = mean(average_temperature, na.rm = TRUE)) %&gt;% ggplot(aes(x = year, y = mean_temp)) + geom_point() ## `summarise()` ungrouping output (override with `.groups` argument) ## Warning: Removed 4 rows containing missing values (geom_point). Let‚Äôs look at monthly temperatures europe_temps %&gt;% group_by(month) %&gt;% summarise(mean_temp = mean(average_temperature, na.rm = TRUE)) %&gt;% ggplot(aes(x = month, y = mean_temp)) + geom_point() + geom_line() ## `summarise()` ungrouping output (override with `.groups` argument) Would this pattern, of temperature increase around June, be the same no matter the continent? 11.6 DATA CHALLENGE 06 Accept data challenge 06 assignment "],["getting-data.html", "Module 12 Getting Data 12.1 Search for data sets 12.2 Extracting data tables from pdf files 12.3 Extracting data tables from websites 12.4 Project Proposal", " Module 12 Getting Data 12.1 Search for data sets There are number of websites that are repositories of data sets. Here‚Äôs a list of some resources: Kaggle Data Sets https://www.kaggle.com/datasets Google Dataset Search https://datasetsearch.research.google.com/ U.S. Department of Education Public Data Listing https://www2.ed.gov/about/data/list.html US Department of Health and Human Services, Datasets &amp; Research Resources https://www.nichd.nih.gov/research/resources/index City of Tucson Open Data https://gisdata.tucsonaz.gov/ 12.2 Extracting data tables from pdf files Many times you won‚Äôt have access to an actual data file. Many institutions make data available in pdf format. Lucky for us, there‚Äôs an R package to extract tables from pdf files. As usual, we need to install the package first. install.packages(&quot;tabulizer&quot;) Remember we need to install a package only once (and updated it once in a while), but every time we want to use it, we need to call it with the library() function. library(tabulizer) library(tidyverse) library(janitor) Let‚Äôs look at UArizona‚Äôs Common Data Set website. For this lesson we will focus on the Common Data Set 2019-2020 First we need to run the extract_tables() function. ua_common_dataset &lt;- extract_tables(&quot;https://uair.arizona.edu/sites/default/files/2019-2020%20CDS_FINAL_060820.pdf&quot;) The extract_tables() function returns a list with all tables that it was able to extract from the pdf file given. Let‚Äôs take a look at the seventh table in the list. ua_common_dataset[[9]] ## [,1] [,2] [,3] ## [1,] &quot;B3&quot; &quot;Certificate/diploma&quot; &quot;1 28&quot; ## [2,] &quot;B3&quot; &quot;Associate degrees&quot; &quot;-&quot; ## [3,] &quot;B3&quot; &quot;Bachelor&#39;s degrees&quot; &quot;7,754&quot; ## [4,] &quot;B3&quot; &quot;Postbachelor&#39;s certificates&quot; &quot;172&quot; ## [5,] &quot;B3&quot; &quot;Master&#39;s degrees&quot; &quot;2,100&quot; ## [6,] &quot;B3&quot; &quot;Post-Master&#39;s certificates&quot; &quot;9&quot; ## [7,] &quot;B3&quot; &quot;Doctoral degrees ‚Äì&quot; &quot;448&quot; ## [8,] &quot;&quot; &quot;research/scholarship&quot; &quot;&quot; ## [9,] &quot;B3&quot; &quot;Doctoral degrees ‚Äì professional&quot; &quot;533&quot; ## [10,] &quot;&quot; &quot;practice&quot; &quot;&quot; ## [11,] &quot;B3&quot; &quot;Doctoral degrees ‚Äì other&quot; &quot;-&quot; It‚Äôs pretty messy. We can start by converting it to an actual data frame. enrollment_data &lt;- ua_common_dataset[[9]] %&gt;% as.data.frame() %&gt;% clean_names() enrollment_data ## v1 v2 v3 ## 1 B3 Certificate/diploma 1 28 ## 2 B3 Associate degrees - ## 3 B3 Bachelor&#39;s degrees 7,754 ## 4 B3 Postbachelor&#39;s certificates 172 ## 5 B3 Master&#39;s degrees 2,100 ## 6 B3 Post-Master&#39;s certificates 9 ## 7 B3 Doctoral degrees ‚Äì 448 ## 8 research/scholarship ## 9 B3 Doctoral degrees ‚Äì professional 533 ## 10 practice ## 11 B3 Doctoral degrees ‚Äì other - We don‚Äôt need the first column enrollment_data &lt;- enrollment_data %&gt;% select(-v1) enrollment_data ## v2 v3 ## 1 Certificate/diploma 1 28 ## 2 Associate degrees - ## 3 Bachelor&#39;s degrees 7,754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2,100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 research/scholarship ## 9 Doctoral degrees ‚Äì professional 533 ## 10 practice ## 11 Doctoral degrees ‚Äì other - We can manually change the column names. colnames(enrollment_data) ## [1] &quot;v2&quot; &quot;v3&quot; colnames(enrollment_data) &lt;- c(&quot;type&quot;, &quot;count&quot;) enrollment_data ## type count ## 1 Certificate/diploma 1 28 ## 2 Associate degrees - ## 3 Bachelor&#39;s degrees 7,754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2,100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 research/scholarship ## 9 Doctoral degrees ‚Äì professional 533 ## 10 practice ## 11 Doctoral degrees ‚Äì other - We can now delete empty rows (which comes from line breaks in the table, you can fix these further if you think it‚Äôs needed). # remove empty rows enrollment_data &lt;- enrollment_data %&gt;% filter(count != &quot;&quot;) enrollment_data ## type count ## 1 Certificate/diploma 1 28 ## 2 Associate degrees - ## 3 Bachelor&#39;s degrees 7,754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2,100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 Doctoral degrees ‚Äì professional 533 ## 9 Doctoral degrees ‚Äì other - We can replace - with zero. # replace - with 0 enrollment_data &lt;- enrollment_data %&gt;% mutate(count = ifelse(count == &quot;-&quot;, 0, count)) enrollment_data ## type count ## 1 Certificate/diploma 1 28 ## 2 Associate degrees 0 ## 3 Bachelor&#39;s degrees 7,754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2,100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 Doctoral degrees ‚Äì professional 533 ## 9 Doctoral degrees ‚Äì other 0 Finally, we convert count to number. # try parse_number enrollment_data %&gt;% mutate(count = parse_number(count)) ## type count ## 1 Certificate/diploma 1 ## 2 Associate degrees 0 ## 3 Bachelor&#39;s degrees 7754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 Doctoral degrees ‚Äì professional 533 ## 9 Doctoral degrees ‚Äì other 0 Remove spaces between numbers. # use gsub to remove spaces (i.e., replace \\\\s with nothing) enrollment_data &lt;- enrollment_data %&gt;% mutate(count = gsub(&quot;\\\\s&quot;, &quot;&quot;, count)) enrollment_data ## type count ## 1 Certificate/diploma 128 ## 2 Associate degrees 0 ## 3 Bachelor&#39;s degrees 7,754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2,100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 Doctoral degrees ‚Äì professional 533 ## 9 Doctoral degrees ‚Äì other 0 Try parse_number again # try parse_number enrollment_data &lt;- enrollment_data %&gt;% mutate(count = parse_number(count)) # check data enrollment_data ## type count ## 1 Certificate/diploma 128 ## 2 Associate degrees 0 ## 3 Bachelor&#39;s degrees 7754 ## 4 Postbachelor&#39;s certificates 172 ## 5 Master&#39;s degrees 2100 ## 6 Post-Master&#39;s certificates 9 ## 7 Doctoral degrees ‚Äì 448 ## 8 Doctoral degrees ‚Äì professional 533 ## 9 Doctoral degrees ‚Äì other 0 We can add info about year, and then process all the other pdf files. # try parse_number enrollment_data &lt;- enrollment_data %&gt;% mutate(year = 2019) # check data enrollment_data ## type count year ## 1 Certificate/diploma 128 2019 ## 2 Associate degrees 0 2019 ## 3 Bachelor&#39;s degrees 7754 2019 ## 4 Postbachelor&#39;s certificates 172 2019 ## 5 Master&#39;s degrees 2100 2019 ## 6 Post-Master&#39;s certificates 9 2019 ## 7 Doctoral degrees ‚Äì 448 2019 ## 8 Doctoral degrees ‚Äì professional 533 2019 ## 9 Doctoral degrees ‚Äì other 0 2019 We can now plot the data. # draw bar plot per type of degree enrollment_data %&gt;% ggplot(aes(x = count, y = reorder(type, count))) + geom_col() + ylab(&quot;&quot;) 12.3 Extracting data tables from websites Other times you will find data available in webpages, or in HTML format. Lucky for us again, there‚Äôs an R package to extract tables from html files. As usual, we need to install the package first. install.packages(&quot;rvest&quot;) Remember we need to install a package only once (and updated it once in a while), but every time we want to use it, we need to call it with the library() function. library(rvest) ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding Let‚Äôs check what tables there are in UArizona‚Äôs wikipedia page. First, we need to read in the html file. uarizona_wiki_html &lt;- read_html(&quot;https://en.wikipedia.org/wiki/University_of_Arizona&quot;) We now parse the html for tables. uarizona_wiki_html %&gt;% html_nodes(&quot;table&quot;) ## {xml_nodeset (19)} ## [1] &lt;table class=&quot;infobox vcard&quot; style=&quot;width:22em&quot;&gt;\\n&lt;caption class=&quot;fn org ... ## [2] &lt;table class=&quot;multicol&quot; role=&quot;presentation&quot; style=&quot;border-collapse: coll ... ## [3] &lt;table class=&quot;infobox&quot; style=&quot;width: 22em&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;&lt;th colspan=&quot;2&quot; ... ## [4] &lt;table class=&quot;wikitable sortable collapsible collapsed&quot; style=&quot;float:rig ... ## [5] &lt;table class=&quot;wikitable sortable collapsible collapsed&quot; style=&quot;float:rig ... ## [6] &lt;table style=&quot;float:right; font-size:85%; margin:10px&quot; class=&quot;wikitable&quot; ... ## [7] &lt;table role=&quot;presentation&quot; class=&quot;mbox-small plainlinks sistersitebox&quot; s ... ## [8] &lt;table class=&quot;nowraplinks hlist mw-collapsible mw-collapsed navbox-inner ... ## [9] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; styl ... ## [10] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; styl ... ## [11] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [12] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [13] &lt;table class=&quot;nowraplinks navbox-subgroup&quot; style=&quot;border-spacing:0&quot;&gt;&lt;tbo ... ## [14] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [15] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [16] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [17] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [18] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [19] &lt;table class=&quot;nowraplinks hlist navbox-inner&quot; style=&quot;border-spacing:0;ba ... Too many tables. We can be specific, and retrieve nodes per class. uarizona_wiki_html %&gt;% html_nodes(&quot;.wikitable&quot;) ## {xml_nodeset (3)} ## [1] &lt;table class=&quot;wikitable sortable collapsible collapsed&quot; style=&quot;float:righ ... ## [2] &lt;table class=&quot;wikitable sortable collapsible collapsed&quot; style=&quot;float:righ ... ## [3] &lt;table style=&quot;float:right; font-size:85%; margin:10px&quot; class=&quot;wikitable&quot;&gt; ... This looks a little better. It looks like the table we want is the third table. # create wiki_tables object wiki_tables &lt;- uarizona_wiki_html %&gt;% html_nodes(&quot;.wikitable&quot;) # transform node into an actual table fall_freshman_stats &lt;- wiki_tables[[3]] %&gt;% html_table(fill = TRUE) # check data fall_freshman_stats ## 2017 2016 2015 2014 2013 ## 1 Applicants 36,166 35,236 32,723 26,481 26,329 ## 2 Admits 28,433 26,961 24,417 20,546 20,251 ## 3 % Admitted 78.6 76.5 74.6 77.5 76.9 ## 4 Enrolled 7,360 7,753 7,466 7,744 6,881 ## 5 Avg GPA 3.43 3.48 3.38 3.37 3.40 ## 6 SAT range* 1015‚Äì1250 1010‚Äì1230 1010‚Äì1230 1000‚Äì1230 990‚Äì1220 ## 7 * SAT out of 1600 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; Tidy it. # first column name is blank colnames(fall_freshman_stats)[1] &lt;- &quot;type&quot; # pivot years fall_freshman_stats &lt;- fall_freshman_stats %&gt;% pivot_longer(cols = &quot;2017&quot;:&quot;2013&quot;, names_to = &quot;year&quot;) # make value a number fall_freshman_stats &lt;- fall_freshman_stats %&gt;% mutate(value = as.numeric(parse_number(value))) # inspect data glimpse(fall_freshman_stats) ## Rows: 35 ## Columns: 3 ## $ type &lt;chr&gt; &quot;Applicants&quot;, &quot;Applicants&quot;, &quot;Applicants&quot;, &quot;Applicants&quot;, &quot;Applic‚Ä¶ ## $ year &lt;chr&gt; &quot;2017&quot;, &quot;2016&quot;, &quot;2015&quot;, &quot;2014&quot;, &quot;2013&quot;, &quot;2017&quot;, &quot;2016&quot;, &quot;2015&quot;,‚Ä¶ ## $ value &lt;dbl&gt; 36166.00, 35236.00, 32723.00, 26481.00, 26329.00, 28433.00, 269‚Ä¶ Plot it. fall_freshman_stats %&gt;% filter(type %in% c(&quot;Applicants&quot;, &quot;Admits&quot;, &quot;Enrolled&quot;)) %&gt;% ggplot(aes(x = year, y = value, color = fct_reorder(type, value, .desc = TRUE))) + geom_point() + theme_bw() + geom_line(aes(group = type)) + labs(y = &quot;student count&quot;, color = &quot;&quot;) 12.4 Project Proposal Project Proposal is due next week (Nov. 05, 2020). "],["data-case-study-3.html", "Module 13 Data Case Study 3 13.1 Adding population info to data frame 13.2 Plotting a map 13.3 DATA CHALLENGE 07", " Module 13 Data Case Study 3 For this case study, we will work with the Great American Beer Festival data set, provided as a tidy tuesday data set. As usual, we first load the libraries we are going to use. library(tidyverse) Then we load the data. beer_awards &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-20/beer_awards.csv&#39;) ## Parsed with column specification: ## cols( ## medal = col_character(), ## beer_name = col_character(), ## brewery = col_character(), ## city = col_character(), ## state = col_character(), ## category = col_character(), ## year = col_double() ## ) Take some time to explore this data set. What questions can you ask? What plots can you draw? 13.1 Adding population info to data frame Whenever you‚Äôre dealing with states or countries, it‚Äôs usually relevant to know the population of the different states/countries. For beer awards, we can assume that the higher the population of a state, the higher the number of breweries, thus the higher the number of awards. Let‚Äôs check if this assumption is correct. First, we need to retrieve information about US states‚Äô population numbers. The usmap package has a statepop data set with that info. First, we need to install usmap and called it with library(). library(usmap) We can now inspect the statepop data frame. glimpse(statepop) ## Rows: 51 ## Columns: 4 ## $ fips &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, ‚Ä¶ ## $ abbr &lt;chr&gt; &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AR&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DE&quot;, &quot;DC&quot;, &quot;FL&quot;, ‚Ä¶ ## $ full &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;C‚Ä¶ ## $ pop_2015 &lt;dbl&gt; 4858979, 738432, 6828065, 2978204, 39144818, 5456574, 359088‚Ä¶ We just need the abbr and pop_2015 columns, but we have to make sure that the state abbreviation column name matches what we have for our beer_awards data. us_pop &lt;- statepop %&gt;% select(&quot;state&quot; = abbr, pop_2015) We are interested in number of beer awards per state, to check whether it correlates with the population size. We need to summarise our beer_awards data to get number of awards per state. beer_awards %&gt;% count(state) ## # A tibble: 52 x 2 ## state n ## &lt;chr&gt; &lt;int&gt; ## 1 Ak 1 ## 2 AK 62 ## 3 AL 6 ## 4 AR 5 ## 5 AZ 92 ## 6 CA 962 ## 7 CO 659 ## 8 CT 16 ## 9 DC 4 ## 10 DE 43 ## # ‚Ä¶ with 42 more rows There are at least two Alaska state codes (Ak and AK). We need to standardize the codes, to upper case, for this count to work correctly. Let‚Äôs fix that in our original data, so we don‚Äôt run into this problem again. beer_awards &lt;- beer_awards %&gt;% mutate(state = toupper(state)) Now we can count number of awards per state. award_count_per_state &lt;- beer_awards %&gt;% count(state) # inspect data glimpse(award_count_per_state) ## Rows: 50 ## Columns: 2 ## $ state &lt;chr&gt; &quot;AK&quot;, &quot;AL&quot;, &quot;AR&quot;, &quot;AZ&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE&quot;, &quot;FL&quot;, &quot;GA‚Ä¶ ## $ n &lt;int&gt; 63, 6, 5, 92, 962, 659, 16, 4, 43, 62, 38, 17, 28, 22, 155, 76,‚Ä¶ We now can add population info from our us_pop data frame to our award_count_per_state data. award_count_per_state &lt;- left_join(award_count_per_state, us_pop) ## Joining, by = &quot;state&quot; # inspect data glimpse(award_count_per_state) ## Rows: 50 ## Columns: 3 ## $ state &lt;chr&gt; &quot;AK&quot;, &quot;AL&quot;, &quot;AR&quot;, &quot;AZ&quot;, &quot;CA&quot;, &quot;CO&quot;, &quot;CT&quot;, &quot;DC&quot;, &quot;DE&quot;, &quot;FL&quot;, ‚Ä¶ ## $ n &lt;int&gt; 63, 6, 5, 92, 962, 659, 16, 4, 43, 62, 38, 17, 28, 22, 155, ‚Ä¶ ## $ pop_2015 &lt;dbl&gt; 738432, 4858979, 2978204, 6828065, 39144818, 5456574, 359088‚Ä¶ Finally, we draw a scatter plot of award count (n) by population size (pop_2015). award_count_per_state %&gt;% ggplot(aes(x = pop_2015, y = n)) + geom_point() 13.2 Plotting a map Whenever you have data with locations, like US states, you can think about plotting data to a geographical map. For the US, that‚Äôs made extra easy with the usmap package. We can now plot a map, using the plot_usmpa function. beer_awards %&gt;% count(state, medal) %&gt;% plot_usmap(data = ., values = &quot;n&quot;) Let‚Äôs make it look nicer by moving the legend right. beer_awards %&gt;% count(state, medal) %&gt;% plot_usmap(data = ., values = &quot;n&quot;) + theme(legend.position = &quot;right&quot;) We can also change the title of the legend. beer_awards %&gt;% count(state, medal) %&gt;% plot_usmap(data = ., values = &quot;n&quot;) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(name = &quot;Award Count&quot;) People usually associate darker colors with higher density or higher values. Let‚Äôs change the colors to define a light color for our low values and a dark color for our high values. Check this documentation for color names in R. beer_awards %&gt;% count(state, medal) %&gt;% plot_usmap(data = ., values = &quot;n&quot;) + theme(legend.position = &quot;right&quot;) + scale_fill_continuous(name = &quot;Award Count&quot;, low = &quot;darkgoldenrod2&quot;, high = &quot;darkgoldenrod4&quot;) 13.3 DATA CHALLENGE 07 Accept data challenge 07 assignment "],["r-markdown.html", "Module 14 R Markdown 14.1 Creating a new R Markdown file 14.2 Chunk Options 14.3 Change YAML header 14.4 Text Formatting 14.5 Reading Data 14.6 Data Tables 14.7 Manual Tables 14.8 Plots 14.9 Knitting PDFs 14.10 Data Challenge 08", " Module 14 R Markdown Module learning objectives: Integrate data analysis code and prose in one document Use basic formatting for prose Use code chunks in R to read data in, and draw tables and plots 14.1 Creating a new R Markdown file To create a new R Markdown file, click on New File then choose R Markdown... Add a title to your document (no need to change anything else). We are outputting a HTML file. To output other formats, you need to install other software. The extension for R Markdown files is .Rmd and when you create a new file, it looks like this: The first few lines in your new R Markdown file is an YAML header beginning and ending with ---. R code blocks start and end with ```, and the language for the code block is in between curly brackets. Note also that the text contains simple markdown formatting (e.g., # for headers and stars for bold text). To knit your R Markdown, click on the knit icon at the top of your file panel. 14.2 Chunk Options You can change or add knitr options for code chunks, here are some examples: include = FALSE/TRUE: sets whether results appear in the finished file. R Markdown runs the code in the chunk regardless of boolean, and the results can be used by other chunks. echo = FALSE/TRUE: sets whether code, but not the results, appear in the finished file. message = FALSE/TRUE: sets whether messages that are generated by code appear in the finished file. warning = FALSE/TRUE: sets whether warnings that are generated by code appear in the finished. fig.cap = ‚Äú‚Ä¶‚Äù adds a caption to graphical results. You can check this R Markdown Reference Guide for a complete list of knitr chunk options. 14.3 Change YAML header You can add a table of contents to the top of your document by changing the YAML header like so: For more options for YAML header options check this book chapter 14.4 Text Formatting # A level-one heading ## A level-two heading ### A level-two heading *italics* or _italics_ **bold** or __bold__ `code` [link description](http://webaddress.com) - bullet list item * bullet list item 1. numbered list item 1. second numbered list item (numbers are incremented automatically in the output) &gt; A block quote. &gt; &gt; &gt; A block quote within a block quote. You can access a R Markdown Cheat Sheet by clicking on Help at the top menu, then choose Cheatsheets. 14.5 Reading Data First, we need to load tidyverse. Then we can read data in. ## Parsed with column specification: ## cols( ## medal = col_character(), ## beer_name = col_character(), ## brewery = col_character(), ## city = col_character(), ## state = col_character(), ## category = col_character(), ## year = col_double(), ## macro_category = col_character(), ## state_area = col_double(), ## region = col_character(), ## state_name = col_character(), ## state_division = col_character() ## ) 14.6 Data Tables Use the function kable() to print out nice tables. Table 14.1: Total number of beer awards per region (1987-2020) Region Total Number of Awards North Central 983 Northeast 537 South 787 West 2659 Whenever you have results with decimals, you can specify the number of digitis to display. Table 14.2: Average number of beer awards per state across regions Region Average Number of Awards North Central 81.92 Northeast 59.67 South 52.47 West 204.54 14.7 Manual Tables You might want to add an explanation table that does not come from your data. You can build tables manually in markdown by using the right dashes. Note that the default is left align. You can use : to change that alignment. | Tables | Are | Cool | |---------------|:-------------:|------:| | row 1 col 1 | row 1 col 2 | $1600 | | row 2 col 1 | row 2 col 2 | $12 | | row 1 col 1 | row 3 col 2 | $1 | Tables Are Cool row 1 col 1 row 1 col 2 $1600 row 2 col 1 row 2 col 2 $12 row 1 col 1 row 3 col 2 $1 14.8 Plots There are also a number of options that you can use for displaying plots nicely. 14.9 Knitting PDFs The easiest way to install TeX in your computer (if you don‚Äôt have TeX/LaTeX installed already) is through the tinytext package. install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() Now you can change html_document to pdf_document to output a pdf instead of a html file. 14.10 Data Challenge 08 Accept data challenge 08 assignment "],["data-case-study-4.html", "Module 15 Data Case Study 4", " Module 15 Data Case Study 4 "],["analysis-reporting.html", "Module 16 Analysis Reporting", " Module 16 Analysis Reporting "],["references.html", "References 16.1 R Markdown 16.2 Including Plots", " References 16.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 16.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "]]
